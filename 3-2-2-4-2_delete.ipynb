{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189297/1551913226.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import interpolate\n",
    "from ccfj import GetStationPairs\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import geopandas as gp\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import maplib\n",
    "from toollib_standard import mathlib\n",
    "from toollib_standard import filelib\n",
    "from toollib_standard import stacklib\n",
    "from toollib_standard import plotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project_repartition_v4.0/output_repar_v9.2_01-01/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('a-project_repar.yml', 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "#name_project = 'project_repartrition/output_repar_03-01/'               # Harmon server\n",
    "name_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_CC_workspace:  ./\n",
      "dir_project_workspace:  ./\n",
      "dir_project:  ./project_repartition_v4.0/output_repar_v9.2_01-01/\n"
     ]
    }
   ],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_project_probes:  ./project/output_FJSJ_17-01/\n",
      "dir_project_targets:  ./project/output_FJSJ_17-02/\n"
     ]
    }
   ],
   "source": [
    "name_project_probes = info_basic['name_project_probes']\n",
    "name_project_targets = info_basic['name_project_targets']\n",
    "dir_project_probes = os.path.join(dir_project_workspace, name_project_probes)\n",
    "dir_project_targets = os.path.join(dir_project_workspace, name_project_targets)\n",
    "print('dir_project_probes: ', dir_project_probes)\n",
    "print('dir_project_targets: ', dir_project_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_all = stainfo['latitude'].tolist() \n",
    "lon_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_subworks = info_basic['key_subworks']\n",
    "dir_partition = dir_project + info_basic['dir_partition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = {}\n",
    "lat = {}\n",
    "lon = {}\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations[key] = stations_this\n",
    "    lat[key] = lat_stations_this.astype(float)\n",
    "    lon[key] = lon_stations_this.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be careful to delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nums_delete = [24,73,90,96,119,137,138,141,190,215,216,219,221,230,234,237,246,296,297,312,321,333,347,371,421,439,441,443,450,467,478,479,488,498,499,515,516,525,526,541,542,543,547,550,552,554,650,651,652,573,574,588,589,594]\n",
    "#nums_delete = [87,139,447,449,487,578]\n",
    "#nums_delete = [32,74,128,131,165,308,350,377,378,380,394,395,396,397,432,451,459,480,481,484,518,529,557,558,559,579,584,586,590]\n",
    "#nums_delete = [47,62]\n",
    "#nums_delete =[61,64,250,268,269,482,560,561,562]\n",
    "#nums_delete = [61,29,27,82,92,94,122,123,130,142,292,305,306,348,393,546]\n",
    "nums_delete = [16,166,173,174,288,413,416,486,500,553,563,565]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16--24-03',\n",
       " '166--42-07',\n",
       " '173--49-07',\n",
       " '174--50-07',\n",
       " '288--37-10',\n",
       " '413--32-13',\n",
       " '416--35-13',\n",
       " '486--22-15',\n",
       " '500--45-15',\n",
       " '553--23-17',\n",
       " '563--46-17',\n",
       " '565--48-17']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_subworks = []\n",
    "for num in nums_delete:\n",
    "    tag = str(num)\n",
    "    for key_subwork in info_basic['key_subworks']:\n",
    "        if tag == key_subwork.split('--')[0]:\n",
    "            key_subworks.append(key_subwork)\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renew information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accept_origin = info_basic_bi['probe_accept']\n",
    "key_probe = info_basic_bi['probe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key_subwork in key_subworks:\n",
    "    target = key_subwork.split('--')[0]\n",
    "    if target in probe_accept_origin.keys():\n",
    "        del probe_accept_origin[target]\n",
    "    if target in key_probe.keys():\n",
    "        del key_probe[key_subwork]\n",
    "info_basic_bi['probe_accept'] = probe_accept_origin\n",
    "info_basic_bi['probe'] = key_probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_subworks:\n",
    "    filePath = dir_partition + key + '.txt'\n",
    "    if os.path.exists(filePath):\n",
    "        os.remove(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_subworks:\n",
    "    del info_basic['key_subworks'][info_basic['key_subworks'].index(key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "   yaml.dump(data=info_basic, stream=f, allow_unicode=True)\n",
    "np.save(dir_project + 'Basic_info.npy', info_basic_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete stack and ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_stack = dir_project + info_basic['dir_stack']\n",
    "dir_ds = dir_project + info_basic['dir_ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = os.listdir(dir_ds)\n",
    "#list_stack = os.listdir(dir_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete  ds_16--24-03.h5\n",
      "delete  ds_166--42-07.h5\n",
      "delete  ds_173--49-07.h5\n",
      "delete  ds_174--50-07.h5\n",
      "delete  ds_288--37-10.h5\n",
      "delete  ds_413--32-13.h5\n",
      "delete  ds_416--35-13.h5\n",
      "delete  ds_486--22-15.h5\n",
      "delete  ds_500--45-15.h5\n",
      "delete  ds_553--23-17.h5\n",
      "delete  ds_563--46-17.h5\n",
      "delete  ds_565--48-17.h5\n"
     ]
    }
   ],
   "source": [
    "for key in key_subworks:\n",
    "    stack_this = key + '_gather_linear.h5'\n",
    "    stack_timewindow = key + '_gather_timewindow.h5'\n",
    "    ds_this = 'ds_'+key +'.h5'\n",
    "    if ds_this in list_ds:\n",
    "        print('delete ', ds_this)\n",
    "        os.remove(dir_ds + ds_this)\n",
    "    #if stack_this in list_stack:\n",
    "    #    print('delete ', stack_this)\n",
    "    #    os.remove(dir_stack + stack_this)\n",
    "    #if stack_timewindow in list_stack:\n",
    "    #    print('delete ', stack_timewindow)\n",
    "    #    os.remove(dir_stack + stack_timewindow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
