{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import interpolate\n",
    "from ccfj import GetStationPairs\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import geopandas as gp\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import maplib\n",
    "from toollib_standard import mathlib\n",
    "from toollib_standard import filelib\n",
    "from toollib_standard import stacklib\n",
    "from toollib_standard import plotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project_repartition_v3.0/output_repar_01-01/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('a-project_repar.yml', 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "#name_project = 'project_repartrition/output_repar_03-01/'               # Harmon server\n",
    "name_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_CC_workspace:  /shdisk/rem2/Harmon/F-J/San/\n",
      "dir_project_workspace:  /shdisk/rem2/Harmon/F-J/San/\n",
      "dir_project:  /shdisk/rem2/Harmon/F-J/San/project_repartition_v3.0/output_repar_01-01/\n"
     ]
    }
   ],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_project_probes:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-01/\n",
      "dir_project_targets:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-02/\n"
     ]
    }
   ],
   "source": [
    "name_project_probes = info_basic['name_project_probes']\n",
    "name_project_targets = info_basic['name_project_targets']\n",
    "dir_project_probes = os.path.join(dir_project_workspace, name_project_probes)\n",
    "dir_project_targets = os.path.join(dir_project_workspace, name_project_targets)\n",
    "print('dir_project_probes: ', dir_project_probes)\n",
    "print('dir_project_targets: ', dir_project_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_all = stainfo['latitude'].tolist() \n",
    "lon_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname = dir_project+ info_basic['stalistname']\n",
    "stainfo_all = pd.read_excel(stalistname,sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname = dir_project + 'subwork_location.xlsx'\n",
    "loc_key = pd.read_excel(stalistname,sheet_name='location')\n",
    "lat_key = loc_key['lat_centroid'].tolist()\n",
    "lon_key = loc_key['lon_centroid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_image:  /shdisk/rem2/Harmon/F-J/San/project_repartition_v3.0/output_repar_01-01/image_01-01/\n"
     ]
    }
   ],
   "source": [
    "dir_image = dir_project+info_basic['dir_image']\n",
    "print('dir_image: ', dir_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be careful to delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nums_delete = [30,247,293,319,353,354,364,365,398,399,428,429,430,431,444,445,448,452,457,459,469,478,479,487,488,509,520,521,523,524,525,568,569,570,572,594,639,673,675]\n",
    "#nums_delete = [29,106,115,383,426,432,444,469,458,506,527,531,549,550,551,552,553,574,583,596,623,641,671,672]\n",
    "nums_delete = [538,604,635,640,668]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538\n",
      "604\n",
      "635\n",
      "640\n",
      "668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['31-15--538', '53-16--604', '40-17--635', '45-17--640', '35-18--668']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_subworks = []\n",
    "for num in nums_delete:\n",
    "    tag = str(num)\n",
    "    #print(tag)\n",
    "    key_subwork0 = [key_subwork for key_subwork in info_basic['key_subworks'] if tag == key_subwork[-len(str(tag)):]]\n",
    "    if key_subwork0 != []:\n",
    "        key_subworks.append(key_subwork0[0])\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renew information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accept_origin = info_basic_bi['probe_accept']\n",
    "key_probe = info_basic_bi['probe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key_subwork in key_subworks:\n",
    "    target = key_subwork.split('--')[0]\n",
    "    if target in probe_accept_origin.keys():\n",
    "        del probe_accept_origin[target]\n",
    "    if target in key_probe.keys():\n",
    "        del key_probe[key_subwork]\n",
    "info_basic_bi['probe_accept'] = probe_accept_origin\n",
    "info_basic_bi['probe'] = key_probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31-15--538\n",
      "53-16--604\n",
      "40-17--635\n",
      "45-17--640\n",
      "35-18--668\n"
     ]
    }
   ],
   "source": [
    "filename = dir_project+'stations_info.xlsx'\n",
    "wb = load_workbook(filename)\n",
    "for key in key_subworks:\n",
    "    #删除sheet\n",
    "    print(key)\n",
    "    try:\n",
    "        \n",
    "        wb.remove(wb[key])\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "wb.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project + 'subwork_location.xlsx'\n",
    "loc_info = pd.read_excel(filename,sheet_name='location')\n",
    "lat_centroid_all_origin = loc_info['lat_centroid'].tolist()\n",
    "lon_centroid_all_origin = loc_info['lon_centroid'].tolist()\n",
    "stations = loc_info['key_subwork'].tolist()\n",
    "for key in key_subworks:\n",
    "    try:\n",
    "        index = stations.index(key)\n",
    "        # 删除对应的行\n",
    "        loc_info.drop(index=index, inplace=True)\n",
    "        # 保存\n",
    "        loc_info.to_excel(filename, sheet_name='location', index=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_subworks:\n",
    "    del info_basic['key_subworks'][info_basic['key_subworks'].index(key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "   yaml.dump(data=info_basic, stream=f, allow_unicode=True)\n",
    "np.save(dir_project + 'Basic_info.npy', info_basic_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete stack and ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_stack = dir_project + info_basic['dir_stack']\n",
    "dir_ds = dir_project + info_basic['dir_ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stack = os.listdir(dir_stack)\n",
    "list_ds = os.listdir(dir_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete  31-15--538_gather_linear.h5\n",
      "delete  31-15--538_gather_timewindow.h5\n",
      "delete  ds_31-15--538.h5\n",
      "delete  53-16--604_gather_linear.h5\n",
      "delete  53-16--604_gather_timewindow.h5\n",
      "delete  ds_53-16--604.h5\n",
      "delete  40-17--635_gather_linear.h5\n",
      "delete  40-17--635_gather_timewindow.h5\n",
      "delete  ds_40-17--635.h5\n",
      "delete  45-17--640_gather_linear.h5\n",
      "delete  45-17--640_gather_timewindow.h5\n",
      "delete  ds_45-17--640.h5\n",
      "delete  35-18--668_gather_linear.h5\n",
      "delete  35-18--668_gather_timewindow.h5\n",
      "delete  ds_35-18--668.h5\n"
     ]
    }
   ],
   "source": [
    "for key in key_subworks:\n",
    "    stack_this = key + '_gather_linear.h5'\n",
    "    stack_timewindow = key + '_gather_timewindow.h5'\n",
    "    ds_this = 'ds_'+key +'.h5'\n",
    "    if stack_this in list_stack:\n",
    "        print('delete ', stack_this)\n",
    "        os.remove(dir_stack + stack_this)\n",
    "    if stack_timewindow in list_stack:\n",
    "        print('delete ', stack_timewindow)\n",
    "        os.remove(dir_stack + stack_timewindow)\n",
    "    if ds_this in list_ds:\n",
    "        print('delete ', ds_this)\n",
    "        os.remove(dir_ds + ds_this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
