{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import interpolate\n",
    "from ccfj import GetStationPairs\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import geopandas as gp\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import maplib\n",
    "from toollib_standard import mathlib\n",
    "from toollib_standard import filelib\n",
    "from toollib_standard import stacklib\n",
    "from toollib_standard import plotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project_repartition_v3.0/output_repar_v9.1_test/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('a-project_repar.yml', 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "#name_project = 'project_repartrition/output_repar_03-01/'               # Harmon server\n",
    "name_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_CC_workspace:  /shdisk/rem2/Harmon/F-J/San/\n",
      "dir_project_workspace:  /shdisk/rem2/Harmon/F-J/San/\n",
      "dir_project:  /shdisk/rem2/Harmon/F-J/San/project_repartition_v3.0/output_repar_v9.1_test/\n"
     ]
    }
   ],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_project_probes:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-01/\n",
      "dir_project_targets:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-02/\n"
     ]
    }
   ],
   "source": [
    "name_project_probes = info_basic['name_project_probes']\n",
    "name_project_targets = info_basic['name_project_targets']\n",
    "dir_project_probes = os.path.join(dir_project_workspace, name_project_probes)\n",
    "dir_project_targets = os.path.join(dir_project_workspace, name_project_targets)\n",
    "print('dir_project_probes: ', dir_project_probes)\n",
    "print('dir_project_targets: ', dir_project_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_all = stainfo['latitude'].tolist() \n",
    "lon_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_subworks = info_basic['key_subworks']\n",
    "dir_partition = dir_project + info_basic['dir_partition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = {}\n",
    "lat = {}\n",
    "lon = {}\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations[key] = stations_this\n",
    "    lat[key] = lat_stations_this.astype(float)\n",
    "    lon[key] = lon_stations_this.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be careful to delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_delete = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1--08-03']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_subworks = []\n",
    "for num in nums_delete:\n",
    "    tag = str(num)\n",
    "    for key_subwork in info_basic['key_subworks']:\n",
    "        if tag == key_subwork.split('--')[0]:\n",
    "            key_subworks.append(key_subwork)\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renew information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accept_origin = info_basic_bi['probe_accept']\n",
    "key_probe = info_basic_bi['probe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key_subwork in key_subworks:\n",
    "    target = key_subwork.split('--')[0]\n",
    "    if target in probe_accept_origin.keys():\n",
    "        del probe_accept_origin[target]\n",
    "    if target in key_probe.keys():\n",
    "        del key_probe[key_subwork]\n",
    "info_basic_bi['probe_accept'] = probe_accept_origin\n",
    "info_basic_bi['probe'] = key_probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_subworks:\n",
    "    filePath = dir_partition + key + '.txt'\n",
    "    if os.path.exists(filePath):\n",
    "        os.remove(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_subworks:\n",
    "    del info_basic['key_subworks'][info_basic['key_subworks'].index(key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "   yaml.dump(data=info_basic, stream=f, allow_unicode=True)\n",
    "np.save(dir_project + 'Basic_info.npy', info_basic_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete stack and ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_stack = dir_project + info_basic['dir_stack']\n",
    "dir_ds = dir_project + info_basic['dir_ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stack = os.listdir(dir_stack)\n",
    "list_ds = os.listdir(dir_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete  1--08-03_gather_linear.h5\n",
      "delete  1--08-03_gather_timewindow.h5\n",
      "delete  ds_1--08-03.h5\n"
     ]
    }
   ],
   "source": [
    "for key in key_subworks:\n",
    "    stack_this = key + '_gather_linear.h5'\n",
    "    stack_timewindow = key + '_gather_timewindow.h5'\n",
    "    ds_this = 'ds_'+key +'.h5'\n",
    "    if stack_this in list_stack:\n",
    "        print('delete ', stack_this)\n",
    "        os.remove(dir_stack + stack_this)\n",
    "    if stack_timewindow in list_stack:\n",
    "        print('delete ', stack_timewindow)\n",
    "        os.remove(dir_stack + stack_timewindow)\n",
    "    if ds_this in list_ds:\n",
    "        print('delete ', ds_this)\n",
    "        os.remove(dir_ds + ds_this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
