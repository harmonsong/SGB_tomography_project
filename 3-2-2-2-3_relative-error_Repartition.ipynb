{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import interpolate\n",
    "#from scipy import interp, arange, exp\n",
    "from ccfj import GetStationPairs\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import geopandas as gp\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import maplib\n",
    "from toollib_standard import mathlib\n",
    "from toollib_standard import filelib\n",
    "from toollib_standard import stacklib\n",
    "from toollib_standard import plotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a-project_repar.yml', 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "#name_project = 'project_repartrition/output_repar_03-01/'               # Harmon server\n",
    "name_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_project_probes = info_basic['name_project_probes']\n",
    "name_project_targets = info_basic['name_project_targets']\n",
    "dir_project_probes = os.path.join(dir_project_workspace, name_project_probes)\n",
    "dir_project_targets = os.path.join(dir_project_workspace, name_project_targets)\n",
    "print('dir_project_probes: ', dir_project_probes)\n",
    "print('dir_project_targets: ', dir_project_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project_probes+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic_probes = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project_probes+'Basic_info.npy'\n",
    "info_basic_bi_probes = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary\n",
    "\n",
    "filename = dir_project_targets+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic_targets = yaml.load(f.read(), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = info_basic_probes['key_subworks']\n",
    "targets = info_basic_targets['key_subworks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_image = dir_project+info_basic['dir_image'] + 'repartition_analysis/'\n",
    "dir_disp_probes  = dir_project_probes + info_basic_probes['dir_disp'] \n",
    "dir_partition_probes = dir_project_probes + info_basic_probes['dir_partition']\n",
    "dir_partition_targets = dir_project_targets + info_basic_targets['dir_partition']\n",
    "print('dir_image: ', dir_image)\n",
    "print('dir_disp: ', dir_disp_probes)\n",
    "if not os.path.exists(dir_image):\n",
    "    os.makedirs(dir_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic_probes['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_stations_all = stainfo['latitude'].tolist() \n",
    "lon_stations_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_probes = {}\n",
    "lat_probes = {}\n",
    "lon_probes = {}\n",
    "for probe in probes:\n",
    "    filepath = dir_partition_probes + str(probe) + '.txt'\n",
    "    stations_this, lat_this, lon_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_probes[probe] = stations_this\n",
    "    lat_probes[probe] = lat_this.astype(float)\n",
    "    lon_probes[probe] = lon_this.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_targets = {}\n",
    "lat_targets = {}\n",
    "lon_targets = {}\n",
    "for target in targets:\n",
    "    filepath = dir_partition_targets + str(target) + '.txt'\n",
    "    stations_this, lat_this, lon_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    station_targets[target] = stations_this\n",
    "    lat_targets[target] = lat_this.astype(float)\n",
    "    lon_targets[target] = lon_this.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = info_basic_bi_probes['f']\n",
    "c = np.linspace(info_basic_probes['c_min'],info_basic_probes['c_max'],info_basic_probes['c_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = np.load('clark_faults.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_probe_all = {}\n",
    "for probe in info_basic_probes['key_subworks']:\n",
    "    file_ds = dir_project_probes  + info_basic_probes['dir_ds'] + 'ds_'+probe+'.h5'\n",
    "    ds = h5py.File(file_ds,'r')\n",
    "    ds_probe_all[probe] = ds['ds_remove'][0]\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_range = info_basic['f_range']\n",
    "f_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probes(target):\n",
    "    global dir_project\n",
    "    global dir_project_probes\n",
    "    global dir_project_targets\n",
    "    global probes\n",
    "    global station_targets\n",
    "\n",
    "    stas = station_targets[target]\n",
    "    # 生成sta的包络\n",
    "    #hull_sta = ConvexHull(stainfo_targets[key_subwork][['Longitude','Latitude']])\n",
    "\n",
    "    stas2key = [sta[1:3]+'-'+sta[3:] for sta in stas]\n",
    "    probes_this = []\n",
    "    for sta in stas2key:\n",
    "        if sta in probes:\n",
    "            probes_this.append(sta)\n",
    "    return probes_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(lon_this,lat_this,lon_stations_all,lat_stations_all):\n",
    "    lon_stations_all = np.array(lon_stations_all)\n",
    "    lat_stations_all = np.array(lat_stations_all)\n",
    "    lon_stations_all = lon_stations_all.reshape(-1,1)\n",
    "    lat_stations_all = lat_stations_all.reshape(-1,1)\n",
    "    dist = np.sqrt((lon_stations_all-lon_this)**2+(lat_stations_all-lat_this)**2)\n",
    "    min_dist = np.min(dist)\n",
    "    index = np.where(dist==min_dist)[0][0]\n",
    "    #print(lon_stations_all)\n",
    "    #print(index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_filter(target):\n",
    "    global flag_err\n",
    "    global flag_plot\n",
    "    global dir_image\n",
    "    global flag_pass\n",
    "    global ds_probe_all\n",
    "    global targets\n",
    "    global probes\n",
    "    global lon_stations_all\n",
    "    global lat_stations_all\n",
    "    global lon_sta\n",
    "    global stations_accept_all\n",
    "    global lon_centroid_all \n",
    "    global lat_centroid_all \n",
    "    global probe_accept_all\n",
    "    global probe_ref_all\n",
    "    # extract all disp\n",
    "    probes_this = extract_probes(target)\n",
    "    stations = set()\n",
    "    lon_centroid_probe = []\n",
    "    lat_centroid_probe = []\n",
    "    for probe in probes_this:\n",
    "        stations_this = stations_probes[probe]\n",
    "        stations = stations.union(set(stations_this))\n",
    "        lon_centroid_probe.append(np.mean( lon_probes[probe]))\n",
    "        lat_centroid_probe.append(np.mean( lat_probes[probe]))\n",
    "    stations = list(stations)\n",
    "    lon_stations_this = []\n",
    "    lat_stations_this = []\n",
    "    for sta in stations:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_this.append(lon_stations_all[index])\n",
    "        lat_stations_this.append(lat_stations_all[index])\n",
    "\n",
    "    # 读取全部autopick dispersion curves\n",
    "    disp_all = {}\n",
    "    for probe in probes_this:\n",
    "        file_ds = dir_project_probes  + info_basic_probes['dir_disp'] + 'autopick_'+probe+'.txt'\n",
    "        disp = np.loadtxt(file_ds,delimiter=',')\n",
    "        index = np.where((disp[:,0]>=f_range[0]) & (disp[:,0]<=f_range[1]))[0]\n",
    "        disp_all[probe] = disp[index,:]\n",
    "\n",
    "    # find central probe and cluster based on it\n",
    "    lon_centroid_this = np.mean(lon_stations_this)\n",
    "    lat_centroid_this = np.mean(lat_stations_this)\n",
    "    index = find_nearest(lon_centroid_this,lat_centroid_this,lon_centroid_probe,lat_centroid_probe)\n",
    "    probe_ref = probes_this[index]\n",
    "    \n",
    "    # original partition\n",
    "    sta_original = set()\n",
    "    for probe in probes_this:\n",
    "        sta_original = sta_original.union(stations_probes[probe])\n",
    "    sta_original = list(sta_original)\n",
    "    lon_stations_original = []\n",
    "    lat_stations_original = []\n",
    "    for sta in sta_original:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_original.append(lon_stations_all[index])\n",
    "        lat_stations_original.append(lat_stations_all[index])\n",
    "    \n",
    "    # calculate relative error between all probes and reference probe\n",
    "    err = []\n",
    "    f_sample = np.linspace(f_range[0],f_range[1],200,endpoint=False)\n",
    "    f_pick_ref = np.loadtxt(dir_disp_probes+'autopick_'+probe_ref+'.txt',delimiter=',')[:,0]\n",
    "    c_pick_ref0 = np.loadtxt(dir_disp_probes+'autopick_'+probe_ref+'.txt',delimiter=',')[:,1]\n",
    "    f = interpolate.interp1d(f_pick_ref,c_pick_ref0,kind='cubic',fill_value='extrapolate')\n",
    "    c_pick_ref = f(f_sample)\n",
    "\n",
    "    c_pick = {}\n",
    "    for probe in probes_this:\n",
    "        c_pick[probe] = []\n",
    "        file_ds = dir_project_probes  + info_basic_probes['dir_disp'] + 'autopick_'+probe+'.txt'\n",
    "        disp = np.loadtxt(file_ds,delimiter=',')\n",
    "        index = np.where((disp[:,0]>=f_range[0]) & (disp[:,0]<=f_range[1]))[0]\n",
    "        disp = disp[index,:]\n",
    "        f_this = disp[:,0]\n",
    "        c_this = disp[:,1]\n",
    "        # 生成插值函数,可以外推\n",
    "        #f = interpolate.interp1d(f_this,c_this,kind='cubic')\n",
    "        f = interpolate.interp1d(f_this,c_this,kind='cubic',fill_value='extrapolate')\n",
    "        c_pick[probe] = f(f_sample)\n",
    "\n",
    "        max_disp = np.sum( np.maximum(np.array(c_pick_ref),np.array(c_pick[probe])) )\n",
    "        abs_err = np.sum( np.abs(np.abs(np.array(c_pick_ref))-np.abs(np.array(c_pick[probe]))) )\n",
    "        err.append(abs_err/max_disp) \n",
    "        \n",
    "    # pass or not\n",
    "    index_pass = np.array(err) < flag_err\n",
    "    probe_pass = np.array(probes_this)[index_pass]\n",
    "    lon_centroid_pass = np.array(lon_centroid_probe)[index_pass]\n",
    "    lat_centroid_pass = np.array(lat_centroid_probe)[index_pass]\n",
    "    err_pass = np.array(err)[index_pass]\n",
    "\n",
    "    # new partition\n",
    "    sta_pass = set()\n",
    "    lon_stations_pass = []\n",
    "    lat_stations_pass = []\n",
    "    for probe in probe_pass:\n",
    "        sta_pass = sta_pass.union(stations_probes[probe])\n",
    "    sta_pass = list(sta_pass)\n",
    "    for sta in sta_pass:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_pass.append(lon_stations_all[index])\n",
    "        lat_stations_pass.append(lat_stations_all[index])\n",
    "    stations_accept_all[target] = sta_pass\n",
    "    lon_centroid_all[target] = np.mean(lon_stations_pass)\n",
    "    lat_centroid_all[target] = np.mean(lat_stations_pass)\n",
    "    if flag_plot:\n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "    \n",
    "        ax = fig.add_subplot(231)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_probe,lat_centroid_probe,c=err,s=20,cmap='jet')\n",
    "        im.set_clim(vmin=0,vmax=0.1)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target + ' error between f='+str(f_range[0])+' and f='+str(f_range[1]))\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax = fig.add_subplot(232)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_pass,lat_centroid_pass,c=err_pass,s=20,cmap='jet')\n",
    "        im.set_clim(vmin=0,vmax=0.1)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target + ' selected probes')\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "\n",
    "        ax = fig.add_subplot(233)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        ax.scatter(lon_stations_original,lat_stations_original,c = 'g',s=20,label = 'original partitions')\n",
    "        # 画空心圆\n",
    "        ax.scatter(lon_stations_pass,lat_stations_pass,c = None,s=30,marker='o',edgecolors='r',label = 'new partitions')\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target+' partition comparison')\n",
    "        ax.legend()\n",
    "\n",
    "        ax = fig.add_subplot(223)\n",
    "        for probe in probes_this:\n",
    "            if probes_this.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.',label='all probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.')\n",
    "        ax.plot(f_sample,c_pick_ref,'r',lw=2,label='central probe')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.set_title(target+\" all probes' dispersion curves\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim(f_range[0],f_range[1])\n",
    "        ax.set_ylim(0.2,2)\n",
    "\n",
    "        ax = fig.add_subplot(224)\n",
    "        for probe in probes_this:\n",
    "            if probes_this.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.',label='all probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.')\n",
    "        probe_pass = list(probe_pass)\n",
    "        for probe in probe_pass:\n",
    "            if probe_pass.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='g',s=2,marker='.',label='picked probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='g',s=2,marker='.')\n",
    "        ax.plot(f_sample,c_pick_ref,'r',lw=2,label='central probe')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.set_title(target+\" dispersion curves accepted\")\n",
    "        ax.set_xlim(f_range[0],f_range[1])\n",
    "        ax.set_ylim(0.2,2)\n",
    "        ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_image+'K-S_filter_'+target+'.png',dpi=100)\n",
    "        plt.close()\n",
    "    \n",
    "    print('cluster_filter: ', target + ' finished, '+str(targets.index(target)+1)+'/'+str(len(targets)))\n",
    "    probe_accept_all[target] = probe_pass\n",
    "    probe_ref_all[target] = probe_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_err = 0.03\n",
    "flag_plot = 0\n",
    "flag_par = 1\n",
    "nThreads = 40\n",
    "probe_accept_all = {}\n",
    "stations_accept_all = {}\n",
    "lon_centroid_all = {}\n",
    "lat_centroid_all = {}\n",
    "probe_ref_all = {}\n",
    "if flag_par == 1 and flag_plot == 0:\n",
    "    pool = ThreadPoolExecutor(max_workers = nThreads)\n",
    "    for target in targets:\n",
    "        pool.submit(cluster_filter, target)\n",
    "    pool.shutdown()\n",
    "else:\n",
    "    for target in targets:\n",
    "        cluster_filter(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_basic['flag_err'] = flag_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze clusters and reunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete void sets\n",
    "key_dels = []\n",
    "for target in targets:\n",
    "    if len(probe_accept_all[target]) < 3:\n",
    "        print('delete: ', target)\n",
    "        del stations_accept_all[target]\n",
    "        del lon_centroid_all[target]\n",
    "        del lat_centroid_all[target]\n",
    "        key_dels.append(target)\n",
    "for key_del in key_dels:\n",
    "    targets.remove(key_del)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lat_lon(target):\n",
    "    global stations_accept_all_new\n",
    "    global lon_centroid_all_new\n",
    "    global lat_centroid_all_new\n",
    "    global stations_accept_all\n",
    "    global lon_centroid_all\n",
    "    global lat_centroid_all\n",
    "    global flag_subwork\n",
    "\n",
    "    for key in stations_accept_all_new.keys():\n",
    "        lon_centroid = lon_centroid_all_new[key]\n",
    "        lat_centroid = lat_centroid_all_new[key]\n",
    "        lon = lon_centroid_all[target]\n",
    "        lat = lat_centroid_all[target]\n",
    "        if lon_centroid == lon and lat_centroid == lat:\n",
    "            print(target + ' merge into ' + str(key))\n",
    "            stations_accept_all_new[key].union(set(stations_accept_all[target]))\n",
    "            return\n",
    "\n",
    "    flag_subwork += 1\n",
    "    new_subwork =  str(flag_subwork)+ '--' + target\n",
    "    stations_accept_all_new[new_subwork] = set(stations_accept_all[target])\n",
    "    lon_centroid_all_new[new_subwork] = lon_centroid_all[target]\n",
    "    lat_centroid_all_new[new_subwork] = lat_centroid_all[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_accept_all_new = {}\n",
    "lon_centroid_all_new = {}\n",
    "lat_centroid_all_new = {}\n",
    "flag_subwork = 0\n",
    "for target in targets:\n",
    "    check_lat_lon(target)\n",
    "key_subworks_new = stations_accept_all_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(lon_stations_all, lat_stations_all,marker='.',color='k',s=1)\n",
    "lon_centroid_target = []\n",
    "lat_centroid_target = []\n",
    "for target in info_basic_targets['key_subworks']:\n",
    "    lon_centroid_target.append(np.mean(lon_targets[target]))\n",
    "    lat_centroid_target.append(np.mean(lat_targets[target]))\n",
    "ax.scatter(lon_centroid_target,lat_centroid_target,c='r',marker='.',s=20)\n",
    "for i in range(len(faults)):\n",
    "    ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'g--')\n",
    "ax.axis('off')\n",
    "    \n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(lon_stations_all, lat_stations_all,marker='.',color='k',s=1)\n",
    "for i in range(len(faults)):\n",
    "    ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'g--')\n",
    "ax.scatter(lon_centroid_all_new.values(),lat_centroid_all_new.values(),c='r',marker='^',s=5)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(dir_image + 'station_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_probe = {}\n",
    "key_probe_accept = {}\n",
    "for key_subwork in key_subworks_new:\n",
    "    key_target = key_subwork[key_subwork.find('--')+2:]\n",
    "    probes_this = extract_probes(key_target)\n",
    "    key_probe[key_subwork] = probes_this\n",
    "    key_probe_accept[key_subwork] = probe_accept_all[key_target]\n",
    "info_basic_bi['probe'] = key_probe\n",
    "info_basic_bi['probe_accept'] = key_probe_accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to new subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_partition = dir_project + 'partition/'\n",
    "info_basic['dir_partition'] = 'partition/'\n",
    "if os.path.exists(dir_partition):\n",
    "    shutil.rmtree(dir_partition)\n",
    "os.makedirs(dir_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_subwork in stations_accept_all_new.keys():\n",
    "    Station = list(stations_accept_all_new[key_subwork])\n",
    "    lat = [lat_stations_all[stalist_all.index(station)] for station in stations_accept_all_new[key_subwork]]\n",
    "    lon =  [lon_stations_all[stalist_all.index(station)] for station in stations_accept_all_new[key_subwork]]\n",
    "    filename = dir_partition + key_subwork + '.txt'\n",
    "    np.savetxt(filename, np.array([Station, lat, lon]).T, fmt='%s', delimiter=' ', header='Station lat lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_basic['key_subworks'] = list(stations_accept_all_new.keys())\n",
    "info_basic['stalistname_all'] = info_basic_probes['stalistname_all']\n",
    "info_basic['stalistname'] = 'stations_info.xlsx'\n",
    "info_basic_bi['probe_accept'] = probe_accept_all\n",
    "info_basic_bi['probe_ref'] = probe_ref_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "   yaml.dump(data=info_basic, stream=f, allow_unicode=True)\n",
    "np.save(dir_project + 'Basic_info.npy', info_basic_bi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
