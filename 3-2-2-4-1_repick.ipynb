{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import interpolate\n",
    "from ccfj import GetStationPairs\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import geopandas as gp\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import maplib\n",
    "from toollib_standard import mathlib\n",
    "from toollib_standard import filelib\n",
    "from toollib_standard import stacklib\n",
    "from toollib_standard import plotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project_repartition_v3.0/output_repar_v9.1_test/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('a-project_repar.yml', 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "#name_project = 'project_repartrition/output_repar_03-01/'               # Harmon server\n",
    "name_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_CC_workspace:  /shdisk/rem2/Harmon/F-J/San/\n",
      "dir_project_workspace:  /shdisk/rem2/Harmon/F-J/San/\n",
      "dir_project:  /shdisk/rem2/Harmon/F-J/San/project_repartition_v3.0/output_repar_v9.1_test/\n"
     ]
    }
   ],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_project_probes:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-01/\n",
      "dir_project_targets:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-02/\n"
     ]
    }
   ],
   "source": [
    "name_project_probes = info_basic['name_project_probes']\n",
    "name_project_targets = info_basic['name_project_targets']\n",
    "dir_project_probes = os.path.join(dir_project_workspace, name_project_probes)\n",
    "dir_project_targets = os.path.join(dir_project_workspace, name_project_targets)\n",
    "print('dir_project_probes: ', dir_project_probes)\n",
    "print('dir_project_targets: ', dir_project_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project_probes+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic_probes = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project_probes+'Basic_info.npy'\n",
    "info_basic_bi_probes = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary\n",
    "\n",
    "filename = dir_project_targets+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic_targets = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "\n",
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = info_basic_probes['key_subworks']\n",
    "targets = info_basic_targets['key_subworks']\n",
    "key_subworks = info_basic['key_subworks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic_probes['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_stations_all = stainfo['latitude'].tolist() \n",
    "lon_stations_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_partition = dir_project + info_basic['dir_partition']\n",
    "dir_partition_probes = dir_project_probes + info_basic_probes['dir_partition']\n",
    "dir_partition_targets = dir_project_targets + info_basic_targets['dir_partition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_probes = {}\n",
    "lat_probes = {}\n",
    "lon_probes = {}\n",
    "for probe in probes:\n",
    "    filepath = dir_partition_probes + str(probe) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_probes[probe] = stations_this\n",
    "    lat_probes[probe] = lat_stations_this.astype(float)\n",
    "    lon_probes[probe] = lon_stations_this.astype(float)\n",
    "\n",
    "stations_targets = {}\n",
    "lat_targets = {}\n",
    "lon_targets = {}\n",
    "for target in targets:\n",
    "    filepath = dir_partition_targets + str(target) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_targets[target] = stations_this\n",
    "    lat_targets[target] = lat_stations_this.astype(float)\n",
    "    lon_targets[target] = lon_stations_this.astype(float)\n",
    "\n",
    "stations = {}\n",
    "lat = {}\n",
    "lon = {}\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations[key] = stations_this\n",
    "    lat[key] = lat_stations_this.astype(float)\n",
    "    lon[key] = lon_stations_this.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = np.load('clark_faults.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_image:  /shdisk/rem2/Harmon/F-J/San/project_repartition_v3.0/output_repar_v9.1_test/image_v9.1_test/repick/\n",
      "dir_disp:  /shdisk/rem2/Harmon/F-J/San/project/output_FJSJ_17-01/autopick17-01//\n",
      "dir_ds:  /shdisk/rem2/Harmon/F-J/San/project_repartition_v3.0/output_repar_v9.1_test/ds_v9.1_test/\n"
     ]
    }
   ],
   "source": [
    "dir_image = dir_project+info_basic['dir_image']+'repick/'\n",
    "if not os.path.exists(dir_image):\n",
    "    os.makedirs(dir_image)\n",
    "dir_disp_probes  = dir_project_probes + info_basic_probes['dir_disp'] \n",
    "dir_ds = dir_project + info_basic['dir_ds']\n",
    "print('dir_image: ', dir_image)\n",
    "print('dir_disp: ', dir_disp_probes)\n",
    "print('dir_ds: ', dir_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accept_all = info_basic_bi['probe_accept']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag_err:  0.05\n",
      "flag_pass:  0.3\n",
      "ks_f_range:  [7.5, 11]\n"
     ]
    }
   ],
   "source": [
    "flag_err = info_basic['flag_err']\n",
    "flag_pass = info_basic['flag_pass']\n",
    "phase_mean = info_basic_bi['phase_mean']\n",
    "ks_f_range = info_basic['ks_f_range']\n",
    "print('flag_err: ', flag_err)\n",
    "print('flag_pass: ', flag_pass)\n",
    "print('ks_f_range: ', ks_f_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probes(target):\n",
    "    global dir_project\n",
    "    global dir_project_probes\n",
    "    global dir_project_targets\n",
    "    global probes\n",
    "    global stations_targets\n",
    "\n",
    "    stas = stations_targets[target]\n",
    "    # 生成sta的包络\n",
    "    #hull_sta = ConvexHull(stainfo_targets[key_subwork][['Longitude','Latitude']])\n",
    "\n",
    "    stas2key = [sta[1:3]+'-'+sta[3:] for sta in stas]\n",
    "    probes_this = []\n",
    "    for sta in stas2key:\n",
    "        if sta in probes:\n",
    "            probes_this.append(sta)\n",
    "    return probes_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(lon_this,lat_this,lon_stations_all,lat_stations_all):\n",
    "    lon_stations_all = np.array(lon_stations_all)\n",
    "    lat_stations_all = np.array(lat_stations_all)\n",
    "    lon_stations_all = lon_stations_all.reshape(-1,1)\n",
    "    lat_stations_all = lat_stations_all.reshape(-1,1)\n",
    "    dist = np.sqrt((lon_stations_all-lon_this)**2+(lat_stations_all-lat_this)**2)\n",
    "    min_dist = np.min(dist)\n",
    "    index = np.where(dist==min_dist)[0][0]\n",
    "    #print(lon_stations_all)\n",
    "    #print(index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lat_lon(key_subwork):\n",
    "    global stations_accept_all_new\n",
    "    global lon_centroid_all_new\n",
    "    global lat_centroid_all_new\n",
    "    global stations_accept_all\n",
    "    global lon_centroid_all\n",
    "    global lat_centroid_all\n",
    "\n",
    "    target = key_subwork.split('--')[1]\n",
    "    stations_accept_all_new[key_subwork] = set(stations_accept_all[target])\n",
    "    lon_centroid_all_new[key_subwork] = lon_centroid_all[target]\n",
    "    lat_centroid_all_new[key_subwork] = lat_centroid_all[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_filter(key_subwork):\n",
    "    global flag_err\n",
    "    global flag_plot\n",
    "    global dir_image\n",
    "    global flag_pass\n",
    "    global ds_probe_all\n",
    "    global dir_ds\n",
    "    global targets\n",
    "    global probes\n",
    "    global lon_stations_all\n",
    "    global lat_stations_all\n",
    "    global lon_sta\n",
    "    global stations_accept_all\n",
    "    global lon_centroid_all \n",
    "    global lat_centroid_all \n",
    "    global probe_accept_all\n",
    "\n",
    "    target = key_subwork.split('--')[1]\n",
    "    # extract all disp\n",
    "    probes_this = extract_probes(target)\n",
    "    stations = set()\n",
    "    lon_centroid_probe = []\n",
    "    lat_centroid_probe = []\n",
    "    for probe in probes_this:\n",
    "        stations_this = stations_probes[probe]\n",
    "        stations = stations.union(set(stations_this))\n",
    "        lon_centroid_probe.append(np.mean( lon_probes[probe]))\n",
    "        lat_centroid_probe.append(np.mean( lat_probes[probe]))\n",
    "    stations = list(stations)\n",
    "    lon_stations_this = []\n",
    "    lat_stations_this = []\n",
    "    for sta in stations:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_this.append(lon_stations_all[index])\n",
    "        lat_stations_this.append(lat_stations_all[index])\n",
    "\n",
    "    # 读取全部autopick dispersion curves\n",
    "    disp_all = {}\n",
    "    for probe in probes_this:\n",
    "        file_ds = dir_project_probes  + info_basic_probes['dir_disp'] + 'autopick_'+probe+'.txt'\n",
    "        disp = np.loadtxt(file_ds,delimiter=',')\n",
    "        index = np.where((disp[:,0]>=ks_f_range[0]) & (disp[:,0]<=ks_f_range[1]))[0]\n",
    "        disp_all[probe] = disp[index,:]\n",
    "    phase_this = [phase_mean[probe] for probe in probes_this]\n",
    "\n",
    "    # find central probe and cluster based on it\n",
    "    lon_centroid_this = np.mean(lon_stations_this)\n",
    "    lat_centroid_this = np.mean(lat_stations_this)\n",
    "    index = find_nearest(lon_centroid_this,lat_centroid_this,lon_centroid_probe,lat_centroid_probe)\n",
    "    c_ref = phase_this[index]\n",
    "    probe_ref = probes_this[index]\n",
    "    \n",
    "    phase_sort = np.sort(phase_this)\n",
    "    index_phase = range(len(phase_sort))\n",
    "    probe_accept = []\n",
    "    phase_accept = []\n",
    "    station_accept = set()\n",
    "    lon_centroid_accept = []\n",
    "    lat_centroid_accept = []\n",
    "    for probe in probes_this:\n",
    "        if phase_mean[probe] >= c_ref*(1-flag_err) and phase_mean[probe] <= c_ref*(1+flag_err):\n",
    "            probe_accept.append(probe)\n",
    "            station_accept = station_accept.union(set(stations_probes[probe]))\n",
    "            lon_centroid_accept.append(np.mean(lon_probes[probe]))\n",
    "            lat_centroid_accept.append(np.mean(lat_probes[probe]))\n",
    "            phase_accept.append(phase_mean[probe])\n",
    "    station_accept = list(station_accept)\n",
    "    lon_stations_accept = []\n",
    "    lat_stations_accept = []\n",
    "    for sta in station_accept:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_accept.append(lon_stations_all[index])\n",
    "        lat_stations_accept.append(lat_stations_all[index])\n",
    "\n",
    "\n",
    "    # ks analysis\n",
    "    index = find_nearest(lon_centroid_this,lat_centroid_this,lon_centroid_accept,lat_centroid_accept)\n",
    "    probe_ref = probe_accept[index]\n",
    "\n",
    "    f_sample = np.linspace(ks_f_range[0],ks_f_range[1],200,endpoint=False)\n",
    "    c_pick = {}\n",
    "    for probe in probes_this:\n",
    "        c_pick[probe] = []\n",
    "        file_ds = dir_project_probes  + info_basic_probes['dir_disp'] + 'autopick_'+probe+'.txt'\n",
    "        disp = np.loadtxt(file_ds,delimiter=',')\n",
    "        index = np.where((disp[:,0]>=ks_f_range[0]) & (disp[:,0]<=ks_f_range[1]))[0]\n",
    "        disp = disp[index,:]\n",
    "        f_this = disp[:,0]\n",
    "        c_this = disp[:,1]\n",
    "        # 生成插值函数,可以外推\n",
    "        #f = interpolate.interp1d(f_this,c_this,kind='cubic')\n",
    "        f = interpolate.interp1d(f_this,c_this,kind='cubic',fill_value='extrapolate')\n",
    "        c_pick[probe] = f(f_sample)\n",
    "\n",
    "    f_pick_ref = np.loadtxt(dir_disp_probes+'autopick_'+probe_ref+'.txt',delimiter=',')[:,0]\n",
    "    c_pick_ref0 = np.loadtxt(dir_disp_probes+'autopick_'+probe_ref+'.txt',delimiter=',')[:,1]\n",
    "    #f = interpolate.interp1d(f_pick_ref,c_pick_ref0,kind='cubic')\n",
    "    f = interpolate.interp1d(f_pick_ref,c_pick_ref0,kind='cubic',fill_value='extrapolate')\n",
    "    c_pick_ref = f(f_sample)\n",
    "\n",
    "    ks_p_values = []\n",
    "    ks_statistic = []\n",
    "    for probe in probe_accept:\n",
    "        ks_p_values.append(ks_2samp(c_pick[probe]-np.mean(c_pick[probe]),c_pick_ref-np.mean(c_pick_ref))[1])\n",
    "        ks_statistic.append(ks_2samp(c_pick[probe]-np.mean(c_pick[probe]),c_pick_ref-np.mean(c_pick_ref))[0])\n",
    "\n",
    "    # original partition\n",
    "    sta_original = set()\n",
    "    for probe in probes_this:\n",
    "        sta_original = sta_original.union(stations_probes[probe])\n",
    "    sta_original = list(sta_original)\n",
    "    lon_stations_original = []\n",
    "    lat_stations_original = []\n",
    "    for sta in sta_original:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_original.append(lon_stations_all[index])\n",
    "        lat_stations_original.append(lat_stations_all[index])\n",
    "\n",
    "\n",
    "    index_pass = np.array(ks_statistic)<flag_pass\n",
    "    probe_pass = np.array(probe_accept)[index_pass]\n",
    "    lon_centroid_pass = np.array(lon_centroid_accept)[index_pass]\n",
    "    lat_centroid_pass = np.array(lat_centroid_accept)[index_pass]\n",
    "    ks_statistic_pass = np.array(ks_statistic)[index_pass]\n",
    "\n",
    "    # new partition\n",
    "    sta_pass = set()\n",
    "    lon_stations_pass = []\n",
    "    lat_stations_pass = []\n",
    "    for probe in probe_pass:\n",
    "        sta_pass = sta_pass.union(stations_probes[probe])\n",
    "    sta_pass = list(sta_pass)\n",
    "    for sta in sta_pass:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_pass.append(lon_stations_all[index])\n",
    "        lat_stations_pass.append(lat_stations_all[index])\n",
    "    stations_accept_all[target] = sta_pass\n",
    "    lon_centroid_all[target] = np.mean(lon_stations_pass)\n",
    "    lat_centroid_all[target] = np.mean(lat_stations_pass)\n",
    "    \n",
    "\n",
    "    if flag_plot == 1:\n",
    "        fig = plt.figure(figsize=(18,15))\n",
    "        ax = fig.add_subplot(332)\n",
    "        ax.scatter(index_phase,phase_sort,c='k',s=20)\n",
    "        ax.axhline(y=c_ref,color='b',ls='--',label='central mean')\n",
    "        ax.axhline(y=c_ref*(1-flag_err),color='r',ls='--',label='tolerance range')\n",
    "        ax.axhline(y=c_ref*(1+flag_err),color='r',ls='--')\n",
    "        ax.set_xlabel('probe index')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.legend()\n",
    "\n",
    "        ax = fig.add_subplot(331)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_probe,lat_centroid_probe,c=phase_this,s=20,cmap='jet')\n",
    "        ax.scatter(lon_centroid_this,lat_centroid_this,c='k',s=60,marker='*',label = 'centroid')\n",
    "        #设置colorbar范围\n",
    "        im.set_clim(vmin=min(phase_this),vmax=max(phase_this))\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target + ' mean phase velocity distribution')\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax.legend()\n",
    "        \"\"\"\n",
    "        print('phase_max: ', max(phase_this))\n",
    "        print('phase_min: ', min(phase_this))\n",
    "        print('phase_bar: ', max(phase_this)-min(phase_this))\n",
    "        print('phase_percent: ', (max(phase_this)-min(phase_this))/np.mean(phase_this)*100, '%')\n",
    "        \"\"\"\n",
    "        ax = fig.add_subplot(333)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_accept,lat_centroid_accept,c=phase_accept,s=20,cmap='jet')\n",
    "        ax.scatter(lon_centroid_this,lat_centroid_this,c='k',s=60,marker='*',label = 'centroid')\n",
    "        im.set_clim(vmin=min(phase_this),vmax=max(phase_this))\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target + ' mean phase velocity distribution after filter')\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax.legend()\n",
    "\n",
    "        ax = fig.add_subplot(334)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_accept,lat_centroid_accept,c=ks_statistic,s=20,cmap='jet')\n",
    "        ax.scatter(lon_centroid_this,lat_centroid_this,c='k',s=60,marker='*',label = 'centroid')\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        im.set_clim(vmin=flag_pass,vmax=flag_pass*2)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target+' K-S value distribution')\n",
    "\n",
    "        ax = fig.add_subplot(335)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_pass,lat_centroid_pass,c=ks_statistic_pass,s=20,cmap='jet')\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        im.set_clim(vmin=flag_pass,vmax=flag_pass*2)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target+' K-S value distribution after filter')\n",
    "        plt.colorbar(im, ax=ax)\n",
    "\n",
    "        ax = fig.add_subplot(336)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        \n",
    "        ax.scatter(lon_stations_original,lat_stations_original,c = 'g',s=20,label = 'original partitions')\n",
    "        # 画空心圆\n",
    "        ax.scatter(lon_stations_pass,lat_stations_pass,c = None,s=30,marker='o',edgecolors='r',label = 'new partitions')\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target+' partition comparison')\n",
    "        ax.legend()\n",
    "\n",
    "        ax = fig.add_subplot(337)\n",
    "        for probe in probes_this:\n",
    "            if probes_this.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.',label='all probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.')\n",
    "        ax.plot(f_sample,c_pick_ref,'r',lw=2,label='central probe')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.set_title(target+\" all probes' dispersion curves\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim(ks_f_range[0],ks_f_range[1])\n",
    "        ax.set_ylim(0.2,2)\n",
    "\n",
    "        ax = fig.add_subplot(338)\n",
    "        for probe in probes_this:\n",
    "            if probes_this.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.',label='all probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.')\n",
    "        probe_pass = list(probe_pass)\n",
    "        for probe in probe_pass:\n",
    "            if probe_pass.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='g',s=2,marker='.',label='picked probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='g',s=2,marker='.')\n",
    "        ax.plot(f_sample,c_pick_ref,'r',lw=2,label='central probe')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.set_title(target+\" dispersion curves accepted\")\n",
    "        ax.set_xlim(ks_f_range[0],ks_f_range[1])\n",
    "        ax.set_ylim(0.2,2)\n",
    "        ax.legend()\n",
    "\n",
    "        ds_this = dir_ds + 'ds_' + key_subwork + '.h5'\n",
    "        if os.path.exists(ds_this):\n",
    "            ax = fig.add_subplot(339)\n",
    "            \n",
    "            ds_this = h5py.File(ds_this,'r')\n",
    "            ds_remove = ds_this['ds_remove'][0]\n",
    "            ds_this.close()\n",
    "            f = info_basic_bi['f']\n",
    "            c = np.linspace(info_basic['c_min'],info_basic['c_max'],info_basic['c_num'])\n",
    "            ax = plotlib.plot_fj(ax,ds_remove,'original F-J',f,c,0,c_map='jet')\n",
    "\n",
    "            #ax.scatter(f_sample,c_pick_ref,c='k', s=1,marker='.')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_image+'repick_'+target+'.png',dpi=100)\n",
    "        plt.close()\n",
    "    print('cluster_filter: ', target + ' finished, '+str(targets.index(target)+1)+'/'+str(len(targets)))\n",
    "    probe_accept_all[target] = probe_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_probe_all = {}\n",
    "for probe in info_basic_probes['key_subworks']:\n",
    "    file_ds = dir_project_probes  + info_basic_probes['dir_ds'] + 'ds_'+probe+'.h5'\n",
    "    ds = h5py.File(file_ds,'r')\n",
    "    ds_probe_all[probe] = ds['ds_remove'][0]\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7--14-03', '9--16-03']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_repick =[7,9,1]\n",
    "\n",
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "key_subworks = []\n",
    "for num in nums_repick:\n",
    "    tag = str(num)\n",
    "    for key_subwork in info_basic['key_subworks']:\n",
    "        if tag == key_subwork.split('--')[0]:\n",
    "            key_subworks.append(key_subwork)\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_filter:  14-03 finished, 8/768\n",
      "cluster_filter:  16-03 finished, 10/768\n"
     ]
    }
   ],
   "source": [
    "flag_err = 0.05\n",
    "flag_pass = 0.3\n",
    "flag_plot = 1\n",
    "ks_f_range = [7.5,14]\n",
    "flag_par = 1\n",
    "nThreads = 40\n",
    "probe_accept_all = {}\n",
    "stations_accept_all = {}\n",
    "lon_centroid_all = {}\n",
    "lat_centroid_all = {}\n",
    "for key_subwork in key_subworks:\n",
    "    cluster_filter(key_subwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7--14-03', '9--16-03']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'key_subworks_repick' in info_basic.keys():\n",
    "    key_subworks_repick = set(info_basic['key_subworks_repick'])\n",
    "    key_subworks_repick = key_subworks_repick.union(set(key_subworks))\n",
    "    key_subworks_repick = list(key_subworks_repick)\n",
    "else:\n",
    "    key_subworks_repick = key_subworks\n",
    "info_basic['key_subworks_repick'] = key_subworks_repick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7--14-03', '9--16-03']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_subworks_repick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_accept_all_new = {}\n",
    "lon_centroid_all_new = {} \n",
    "lat_centroid_all_new = {}\n",
    "for key_subwork in key_subworks:\n",
    "    check_lat_lon(key_subwork)\n",
    "\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary\n",
    "\n",
    "probe_accept_origin = info_basic_bi['probe_accept']\n",
    "key_probe = info_basic_bi['probe']\n",
    "for key_subwork in key_subworks:\n",
    "    target = key_subwork.split('--')[1]\n",
    "    probe_accept_origin[target] = probe_accept_all[target]\n",
    "    probes_this = extract_probes(target)\n",
    "    key_probe[key_subwork] = probes_this\n",
    "info_basic_bi['probe_accept'] = probe_accept_origin\n",
    "info_basic_bi['probe'] = key_probe\n",
    "\n",
    "for key in key_subworks:\n",
    "    Station = list(stations_accept_all_new[key_subwork])\n",
    "    lat = [lat_stations_all[stalist_all.index(station)] for station in stations_accept_all_new[key_subwork]]\n",
    "    lon =  [lon_stations_all[stalist_all.index(station)] for station in stations_accept_all_new[key_subwork]]\n",
    "    filename = dir_partition + key_subwork + '.txt'\n",
    "    np.savetxt(filename, np.array([Station, lat, lon]).T, fmt='%s', delimiter=' ', header='Station lat lon')\n",
    "\n",
    "with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(data=info_basic, stream=f, allow_unicode=True)\n",
    "np.save(dir_project + 'Basic_info.npy', info_basic_bi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
