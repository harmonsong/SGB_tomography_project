{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ccfj_rc\n",
    "from ccfj_rc import GetStationPairs\n",
    "from geopy.distance import great_circle\n",
    "import folium\n",
    "import h5py\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import plotlib\n",
    "from toollib_standard import mathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_project = 1 # 0--regular ; 1--repartition; 2--voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_project == 0:\n",
    "    file_project = 'a-project.yml'\n",
    "elif flag_project == 1:\n",
    "    file_project = 'a-project_repar.yml'\n",
    "elif flag_project == 2:\n",
    "    file_project = 'a-project_voro.yml'\n",
    "    \n",
    "with open(file_project, 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ds = dir_project + info_basic['dir_ds']\n",
    "dir_CC = dir_CC_workspace + info_basic['name_CC']\n",
    "dir_partition = dir_project + info_basic['dir_partition']\n",
    "dir_image = dir_project + info_basic['dir_image'] + 'regular_FJ/'\n",
    "if os.path.exists(dir_image) == False:\n",
    "    os.makedirs(dir_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = dir_project+dir_inv_dispernet+ 'inv3.txt'\n",
    "#nums = np.loadtxt(filename,dtype=int)\n",
    "nums = [1]\n",
    "key_subworks_all = info_basic['key_subworks']\n",
    "key_subworks = []\n",
    "for num in nums:\n",
    "    for key_subwork in key_subworks_all:\n",
    "        if str(num) == key_subwork.split('--')[0]:\n",
    "            key_subworks.append(key_subwork)\n",
    "            break\n",
    "key_subworks = info_basic['key_subworks']\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_all = stainfo['latitude'].tolist() \n",
    "lon_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = {}\n",
    "lat = {}\n",
    "lon = {}\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations[key] = stations_this\n",
    "    lat[key] = lat_stations_this.astype(float)\n",
    "    lon[key] = lon_stations_this.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ds = dir_project + info_basic['dir_ds']\n",
    "dir_disp = dir_project + info_basic['dir_inv_dispernet'] + 'data/'\n",
    "dir_CC = dir_CC_workspace+info_basic['name_CC']\n",
    "dir_partition = dir_project + info_basic['dir_partition']\n",
    "stalistname_all = info_basic['stalistname_all']\n",
    "dir_image = dir_project + info_basic['dir_image'] + 'bootstrap/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_boot = 'dir_bootstrap/'\n",
    "dir_boot = dir_project + name_boot\n",
    "if os.path.exists(dir_boot) == False:\n",
    "    os.makedirs(dir_boot)\n",
    "    info_basic['name_boot'] = name_boot\n",
    "    with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(data=info_basic, stream=f, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncffile = h5py.File(dir_CC + 'gather_all.h5','r')\n",
    "ncfs = ncffile['ncfs'][:]\n",
    "f = ncffile['f'][:]\n",
    "count_all = ncffile['count'][:]\n",
    "ncffile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_window_filter(t,ncfst0,r,v_min,t0,a):\n",
    "    ncfst = ncfst0.copy()\n",
    "    for i in range(len(ncfst)):\n",
    "        tag = r[i]/v_min\n",
    "        #print(t0,tag)\n",
    "        t1 = t[t>-tag-t0][t[t>-tag-t0]< tag+t0]\n",
    "        start = np.where(t == t1[0])[0][0]\n",
    "        end = np.where(t == t1[-1])[0][0]\n",
    "        ncfst[i][start:end+1]= ncfst[i][start:end+1]* np.exp(-a*np.abs((tag-np.abs(t1))))\n",
    "    return ncfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stack(ncfs_sum_linear,r):\n",
    "    global dir_project\n",
    "    global key_subworks\n",
    "    global info_basic\n",
    "    global flag_save\n",
    "    global v_tag\n",
    "    global t0\n",
    "    global a\n",
    "\n",
    "    f0 = info_basic_bi['f']\n",
    "    dt = 1/np.max(f0)\n",
    "    t = (np.linspace(-len(f0)-1,len(f0)-1,2*(len(f0)-1))+0.5)*dt/2\n",
    "    ncfst_linear = mathlib.freq_time(ncfs_sum_linear)\n",
    "    ncfst1 = time_window_filter(t,ncfst_linear,r,v_tag,t0,a)\n",
    "    ncfs_sum_remove = mathlib.time_freq(ncfst1)\n",
    "    return ncfs_sum_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_linear_all = {}\n",
    "ds_remove_all = {}\n",
    "index_ncfs = {}\n",
    "rs = {}\n",
    "for key_subwork in key_subworks:\n",
    "    print(key_subwork+ '; '+str(key_subworks.index(key_subwork)+1)+'/'+str(len(key_subworks))+' subworks.')\n",
    "    ds = h5py.File(dir_ds+'ds_'+str(key_subwork)+'.h5', 'r')\n",
    "    ds_linear_all[key_subwork] = ds['ds_linear'][0]\n",
    "    ds_remove_all[key_subwork] = ds['ds_remove'][0]\n",
    "    index_ncfs[key_subwork] = list(ds['index_ncfs'])\n",
    "    rs[key_subwork] = np.array(ds['r'])\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_plot = 1\n",
    "flag_save = 0\n",
    "# remove parameter\n",
    "v_tag = info_basic['v_tag']\n",
    "t0 = info_basic['t0']\n",
    "a = info_basic['a']\n",
    "# bootstrap parameter\n",
    "inter_f = 10\n",
    "start_f = 5\n",
    "end_f = 20\n",
    "random_num=20\n",
    "f0 = info_basic_bi['f']\n",
    "index_start_f = np.argmin(np.abs(f0-start_f)) # 300\n",
    "index_end_f = np.argmin(np.abs(f0-end_f)) # 700\n",
    "c = np.linspace(info_basic['c_min'],info_basic['c_max'],info_basic['c_num'])\n",
    "nf = len(f)\n",
    "nc = len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start00 = time.time()\n",
    "for key_subwork in key_subworks:\n",
    "    start0 = time.time()\n",
    "    print('Start '+ key_subwork+' ...')\n",
    "    ds_this = ds_remove_all[key_subwork]\n",
    "    index = index_ncfs[key_subwork]\n",
    "    ncfs_sum_linear = ncfs[index,:]\n",
    "    r = rs[key_subwork]\n",
    "    nr = len(r)\n",
    "    nr_selected = int(nr/2)\n",
    "    \n",
    "    stalist = stations[key_subwork]\n",
    "    lat_this = lat[key_subwork]\n",
    "    lon_this = lon[key_subwork]\n",
    "    nsta = len(stalist)\n",
    "    StationPairs = GetStationPairs(nsta)\n",
    "    nPairs = int(len(StationPairs)/2)\n",
    "    ncfs_sum_remove = remove_stack(ncfs_sum_linear,r)\n",
    "    ds_remove = plotlib.smooth_ds(ncfs_sum_remove)\n",
    "\n",
    "    # random pick\n",
    "    random_indx = np.array(random.sample(range(nr), nr_selected))\n",
    "    ncfs_new = ncfs_sum_remove[random_indx,:]\n",
    "    r1 = r[random_indx]\n",
    "    indx = np.argsort(r1)\n",
    "    r1 = r1[indx]\n",
    "    ncfs_new = ncfs_new[indx]\n",
    "\n",
    "    # read dispersion curves\n",
    "    file_curve = dir_disp + 'ds_'+key_subwork+'curve.txt'\n",
    "    curve = np.loadtxt(file_curve)\n",
    "    curve0 = curve[curve[:,2]==0,:]\n",
    "    c0 = np.interp(f0, curve0[:,0], curve0[:,1])\n",
    "    fr = f0[index_start_f:index_end_f:inter_f]\n",
    "    cr = c0[index_start_f:index_end_f:inter_f]\n",
    "\n",
    "    # redefine calculate zone\n",
    "    c = np.zeros(nf*2)\n",
    "    for i in range(nf):\n",
    "        c[i*2] = c0[i]*0.93\n",
    "        c[i*2+1] = c0[i]*1.07\n",
    "    F = np.zeros([nc,nf])\n",
    "    C = np.zeros([nc,nf])\n",
    "    for i in range(nf):\n",
    "        for j in range(nc):\n",
    "            F[j,i] = f0[i]\n",
    "            C[j,i] = c[i*2]+(c[i*2+1]-c[i*2])/(nc-1)*j\n",
    "    ds_new = ccfj_rc.fj_noise(np.real(ncfs_sum_remove),r,c,f0,nc,func=1)\n",
    "\n",
    "    # center max\n",
    "    cr0 = np.zeros_like(fr)\n",
    "    ds_smooth = plotlib.smooth_ds(ds_new)\n",
    "    ds_smooth = np.nan_to_num(ds_smooth)\n",
    "    for i in range(len(fr)):\n",
    "        j = index_start_f+i*inter_f\n",
    "        idx = np.argmax(ds_smooth[:,j])\n",
    "        cr0[i] = C[idx,j]\n",
    "\n",
    "    # calculate bootstrap\n",
    "    cr1 = np.zeros([random_num,len(fr)])\n",
    "    for i in range(random_num):\n",
    "        #print(i)\n",
    "        random_indx = np.array(random.sample(range(nr), nr_selected))\n",
    "        ncfs1 = ncfs_sum_remove[random_indx,:]\n",
    "        r1 = r[random_indx]\n",
    "        indx = np.argsort(r1)\n",
    "        r1 = r1[indx]\n",
    "        ncfs1 = ncfs1[indx]\n",
    "        ds = ccfj_rc.fj_noise(np.real(ncfs1),r1,c,f,nc,func=1)\n",
    "        for j in range(len(fr)):\n",
    "            k = index_start_f+j*inter_f\t\n",
    "            idx = np.argmax(ds[:,k])\n",
    "            cr1[i,j] = C[idx,k]\n",
    "    \n",
    "    # Save\n",
    "    if flag_save:\n",
    "        outname = key_subwork+'_bootstrap.h5'\n",
    "        if os.path.exists(dir_boot+outname):\n",
    "            os.remove(dir_boot+outname)\n",
    "        h5file = h5py.File(dir_boot+outname,'w')\n",
    "        h5file.create_dataset('fr0',data=fr)\n",
    "        h5file.create_dataset('cr0',data=cr0)\n",
    "        h5file.create_dataset('cr1',data=cr1)\n",
    "        h5file.create_dataset('F',data=F)\n",
    "        h5file.create_dataset('C',data=C)\n",
    "        h5file.create_dataset('ds_remove',data=ds_remove)\n",
    "    \n",
    "    print('Finish *** '+ key_subwork + ' ***,',time.time()-start0, ' seconds; time since start:', time.time()-start00, ' seconds. Proceeded '+str(key_subworks.index(key_subwork)+1)+'/'+str(len(key_subworks))+' subworks.')\n",
    "\n",
    "    # plot\n",
    "    if flag_plot == 1:\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        plt.pcolormesh(F,C,ds_smooth,vmin=0.5,cmap='jet')\n",
    "        for i in range(random_num):\n",
    "            plt.scatter(fr,cr1[i,:],c='gray',marker='.',s = 3)\n",
    "        plt.plot(fr,cr0,'k^',markersize=3)\n",
    "\n",
    "        c_max = []\n",
    "        c_min = []\n",
    "        for i in range(len(fr)):\n",
    "            c_max.append(np.max(cr1[:,i]))\n",
    "            c_min.append(np.min(cr1[:,i]))\n",
    "        plt.plot(fr,c_max,'b--')\n",
    "        plt.plot(fr,c_min,'b--')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Phase Velocity (km/s)')\n",
    "        plt.title(key_subwork)\n",
    "        plt.tight_layout()\n",
    "        if os.path.exists(dir_image)==False:\n",
    "            os.makedirs(dir_image)\n",
    "        plt.savefig(dir_image+key_subwork+'_bootstrap')\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
