{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import geopandas as gp\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.interpolate import griddata\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from pyproj import Proj\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import plotlib\n",
    "from toollib_standard import mathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_project = 1 # 0--regular; 1--repartrition\n",
    "flag_smooth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_project == 0:\n",
    "    file_project = 'a-project.yml'\n",
    "elif flag_project == 1:\n",
    "    file_project = 'a-project_repar.yml'\n",
    "elif flag_project == 2:\n",
    "    file_project = 'a-project_voro.yml'\n",
    "    \n",
    "with open(file_project, 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "\n",
    "#name_project = 'project/output_FJSJ_16-01/'               \n",
    "#name_project = 'project_repartrition/repartrition_01-03/'               \n",
    "#name_project = 'project_voronoi/voronoi_01-03/'         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_model_fund = dir_project + info_basic['rdir_model_fund']\n",
    "dir_model = dir_project + info_basic['rdir_model']\n",
    "dir_inv = dir_project + info_basic['rdir_inv_BFGS']\n",
    "dir_image = dir_project + info_basic['rdir_image']+'Vs_compare/'\n",
    "if not os.path.exists(dir_image):\n",
    "    os.makedirs(dir_image)\n",
    "dir_partition = dir_project + info_basic['rdir_partition']\n",
    "key_subworks = info_basic['key_subworks']\n",
    "M = len(key_subworks)\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = mathlib.GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist() \n",
    "lat_stations_all =  stainfo['latitude'].tolist() \n",
    "lon_stations_all =  stainfo['longitude'].tolist() \n",
    "elevation_stations_all = stainfo['elevation'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_partition = {}\n",
    "lat_stations_partition = {}\n",
    "lon_stations_partition = {}\n",
    "lat_centroid_partition = []\n",
    "lon_centroid_partition = []\n",
    "num_stations = []\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_partition[key] = stations_this\n",
    "    lat_stations_partition[key] = lat_stations_this.astype(float)\n",
    "    lon_stations_partition[key] = lon_stations_this.astype(float)\n",
    "    num_stations.append(len(stations_this))\n",
    "    lat_centroid_partition.append(np.mean(lat_stations_this.astype(float)))\n",
    "    lon_centroid_partition.append(np.mean(lon_stations_this.astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = np.load('clark_faults.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate 3D structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz = 5\n",
    "N = 40\n",
    "num_x = 100\n",
    "num_y = 100\n",
    "type_interp = 'linear'\n",
    "flag_save = 1 # 1--npz; 2--mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data\n",
    "struc = {}\n",
    "struc_fund = {}\n",
    "flag = 0\n",
    "for key_subwork in key_subworks:\n",
    "    file_model = dir_model + 'model_'+key_subwork+'.txt'\n",
    "    file_model_fund = dir_model_fund + 'model_'+key_subwork+'.txt'\n",
    "    model = np.loadtxt(file_model)\n",
    "    model_fund = np.loadtxt(file_model_fund)\n",
    "    struc[key_subwork] = {}\n",
    "    struc[key_subwork]['layer'] = model[:, 0]\n",
    "    struc[key_subwork]['z'] = model[:, 1]\n",
    "    struc[key_subwork]['rho'] = model[:, 2]\n",
    "    struc[key_subwork]['vs'] = model[:, 3]\n",
    "    struc[key_subwork]['vp'] = model[:, 4]\n",
    "    struc[key_subwork]['std'] = model[:, 5]\n",
    "    struc_fund[key_subwork] = {}\n",
    "    struc_fund[key_subwork]['layer'] = model_fund[:, 0]\n",
    "    struc_fund[key_subwork]['z'] = model_fund[:, 1]\n",
    "    struc_fund[key_subwork]['rho'] = model_fund[:, 2]\n",
    "    struc_fund[key_subwork]['vs'] = model_fund[:, 3]\n",
    "    struc_fund[key_subwork]['vp'] = model_fund[:, 4]\n",
    "    struc_fund[key_subwork]['std'] = model_fund[:, 5]\n",
    "    flag += 1\n",
    "    print('Read structure model: %s (%d/%d)' % (key_subwork, flag, M))\n",
    "\n",
    "vs_horizon = {}\n",
    "vs_horizon_fund = {}\n",
    "for i in range(N):\n",
    "    vs_horizon[i] = []\n",
    "    vs_horizon_fund[i] = []\n",
    "    for key_subwork in key_subworks:\n",
    "        vs_horizon[i].append(struc[key_subwork]['vs'][i])\n",
    "        vs_horizon_fund[i].append(struc_fund[key_subwork]['vs'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = np.column_stack((lon_stations_all, lat_stations_all))\n",
    "points = np.column_stack((lon_centroid_partition, lat_centroid_partition))\n",
    "hull = ConvexHull(points)\n",
    "polygon = Polygon(points[hull.vertices])\n",
    "index_sta = []\n",
    "lon_stations_in = []\n",
    "lat_stations_in = []\n",
    "stalist_in = []\n",
    "elevation_stations_in = []\n",
    "for i in range(len(lon_stations_all)):\n",
    "    if polygon.contains(Point(lon_stations_all[i], lat_stations_all[i])):\n",
    "        index_sta.append(i)\n",
    "        stalist_in.append(stalist_all[i])\n",
    "        lon_stations_in.append(lon_stations_all[i])\n",
    "        lat_stations_in.append(lat_stations_all[i])\n",
    "        elevation_stations_in.append(elevation_stations_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affine_transform(original_points, target_points):\n",
    "    A_matrix = np.array([[original_points[0][0], original_points[0][1], 1, 0, 0, 0],\n",
    "                         [0, 0, 0, original_points[0][0], original_points[0][1], 1],\n",
    "                         [original_points[1][0], original_points[1][1], 1, 0, 0, 0],\n",
    "                         [0, 0, 0, original_points[1][0], original_points[1][1], 1],\n",
    "                         [original_points[2][0], original_points[2][1], 1, 0, 0, 0],\n",
    "                         [0, 0, 0, original_points[2][0], original_points[2][1], 1]])\n",
    "\n",
    "    A1_B1_C1 = np.array([target_points[0][0], target_points[0][1], target_points[1][0], target_points[1][1], target_points[2][0], target_points[2][1]])\n",
    "\n",
    "    coefficients = np.linalg.solve(A_matrix, A1_B1_C1)\n",
    "\n",
    "    affine_matrix = np.array([[coefficients[0], coefficients[1], coefficients[2]],\n",
    "                               [coefficients[3], coefficients[4], coefficients[5]],\n",
    "                               [0, 0, 1]])\n",
    "\n",
    "    return affine_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine transformation\n",
    "lon_stations_all_new = []\n",
    "lat_stations_all_new = []\n",
    "for sta in stalist_all:\n",
    "    if int(sta[1:3]) <= 60:\n",
    "        lon_stations_all_new.append(lon_stations_all[stalist_all.index(sta)])\n",
    "        lat_stations_all_new.append(lat_stations_all[stalist_all.index(sta)])\n",
    "refs = ['R0101','R6001','R6020']\n",
    "lon_refs = [lon_stations_all[stalist_all.index(ref)] for ref in refs]\n",
    "lat_refs = [lat_stations_all[stalist_all.index(ref)] for ref in refs]\n",
    "loc_refs = np.column_stack([lon_refs,lat_refs])\n",
    "loc_refs_new = np.array([[0,0],[600,0],[600,600]])\n",
    "\n",
    "affine_matrix = compute_affine_transform(loc_refs, loc_refs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all stations\n",
    "x_stations_all = []\n",
    "y_stations_all = []\n",
    "for i in range(len(lon_stations_all)):\n",
    "    loc_sta = np.array([lon_stations_all[i],lat_stations_all[i],1])\n",
    "    loc_sta_new = np.dot(affine_matrix,loc_sta)\n",
    "    x_stations_all.append(loc_sta_new[0])\n",
    "    y_stations_all.append(loc_sta_new[1])\n",
    "np.savetxt(dir_project + 'stations_all_trans.txt', np.column_stack((stalist_all,x_stations_all, y_stations_all)), fmt='%s')\n",
    "# all stations\n",
    "x_stations_all_new = []\n",
    "y_stations_all_new = []\n",
    "for i in range(len(lon_stations_all_new)):\n",
    "    loc_sta = np.array([lon_stations_all_new[i],lat_stations_all_new[i],1])\n",
    "    loc_sta_new = np.dot(affine_matrix,loc_sta)\n",
    "    x_stations_all_new.append(loc_sta_new[0])\n",
    "    y_stations_all_new.append(loc_sta_new[1])\n",
    "# in stations\n",
    "x_stations_in = []\n",
    "y_stations_in = []\n",
    "for i in range(len(lon_stations_in)):\n",
    "    loc_sta = np.array([lon_stations_in[i],lat_stations_in[i],1])\n",
    "    loc_sta_new = np.dot(affine_matrix,loc_sta)\n",
    "    x_stations_in.append(loc_sta_new[0])\n",
    "    y_stations_in.append(loc_sta_new[1])\n",
    "np.savetxt(dir_project +'stations_in_trans.txt', np.column_stack((stalist_in,x_stations_in, y_stations_in)), fmt='%s')\n",
    "# faults\n",
    "x_faults = {}\n",
    "y_faults = {}\n",
    "faults_trans = {}   \n",
    "for i in range(len(faults)):\n",
    "    x_faults['clark'+str(i+1)] = []\n",
    "    y_faults['clark'+str(i+1)] = []\n",
    "    faults_trans['clark'+str(i+1)] = {}\n",
    "    for j in range(len(faults['clark'+str(i+1)]['lon'])):\n",
    "        loc_fault = np.array([faults['clark'+str(i+1)]['lon'][j],faults['clark'+str(i+1)]['lat'][j],1])\n",
    "        loc_fault_new = np.dot(affine_matrix,loc_fault)\n",
    "        x_faults['clark'+str(i+1)].append(loc_fault_new[0])\n",
    "        y_faults['clark'+str(i+1)].append(loc_fault_new[1])\n",
    "        faults_trans['clark'+str(i+1)]['x'] = x_faults['clark'+str(i+1)]\n",
    "        faults_trans['clark'+str(i+1)]['y'] = y_faults['clark'+str(i+1)]\n",
    "np.save(dir_project +'clark_faults_trans.npy', faults_trans)\n",
    "# partitions\n",
    "x_centroid_partition = []\n",
    "y_centroid_partition = []\n",
    "for i in range(len(lon_centroid_partition)):\n",
    "    loc_centroid = np.array([lon_centroid_partition[i],lat_centroid_partition[i],1])\n",
    "    loc_centroid_new = np.dot(affine_matrix,loc_centroid)\n",
    "    x_centroid_partition.append(loc_centroid_new[0])\n",
    "    y_centroid_partition.append(loc_centroid_new[1])\n",
    "np.savetxt(dir_project + 'partition_trans.txt', np.column_stack((key_subworks,x_centroid_partition, y_centroid_partition)), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(lon_stations_all_new, lat_stations_all_new, c='k', label='Original',s = 1)\n",
    "for ref in refs:\n",
    "    ax.scatter(lon_stations_all[stalist_all.index(ref)], lat_stations_all[stalist_all.index(ref)], c='g', label='Original',s = 10)\n",
    "for i in range(len(faults)):\n",
    "    ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(x_stations_all_new, y_stations_all_new, c='k', label='Original',s = 1)\n",
    "for i in range(len(faults)):\n",
    "    ax.plot(x_faults['clark'+str(i+1)], y_faults['clark'+str(i+1)], 'k')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_xlim(-50,650)\n",
    "ax.set_ylim(-50,650)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 坐标逆变换\n",
    "affine_matrix_inv = np.linalg.inv(affine_matrix)\n",
    "# stations in\n",
    "lon_inter_in = np.zeros(len(x_stations_in))\n",
    "lat_inter_in = np.zeros(len(x_stations_in))\n",
    "for i in range(len(x_stations_in)):\n",
    "    loc_sta_new = np.array([x_stations_in[i],y_stations_in[i],1])\n",
    "    loc_sta = np.dot(affine_matrix_inv,loc_sta_new)\n",
    "    lon_inter_in[i] = loc_sta[0]\n",
    "    lat_inter_in[i] = loc_sta[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate grid\n",
    "xx = np.linspace(np.min(x_centroid_partition), np.max(y_centroid_partition), num_x)\n",
    "yy = np.linspace(np.min(y_centroid_partition), np.max(y_centroid_partition), num_y)\n",
    "A,B = np.meshgrid(xx, yy)\n",
    "X_star = np.hstack((A.flatten()[:,None], B.flatten()[:,None]))\n",
    "x_grid = X_star[:,0]\n",
    "y_grid = X_star[:,1]\n",
    "\n",
    "points = np.column_stack((x_centroid_partition, y_centroid_partition))\n",
    "hull = ConvexHull(points)\n",
    "polygon = Polygon(points[hull.vertices])\n",
    "index = []\n",
    "x_inter_in = []\n",
    "y_inter_in = []\n",
    "for i in range(len(x_grid)):\n",
    "    if polygon.contains(Point(x_grid[i], y_grid[i])):\n",
    "        index.append(i)\n",
    "        x_inter_in.append(x_grid[i])\n",
    "        y_inter_in.append(y_grid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_inter_fund_horizon_in = np.zeros((N,len(index)))\n",
    "vs_inter_horizon_in = np.zeros((N,len(index)))\n",
    "ele_inter = np.zeros((N,len(index)))\n",
    "for i in range(N):\n",
    "    print('Interpolating horizon %d/%d' % (i+1,N))\n",
    "    OK = OrdinaryKriging(x_centroid_partition, y_centroid_partition, vs_horizon[i], variogram_model=type_interp,nlags=3)\n",
    "    zz,ss = OK.execute('grid', xx, yy)\n",
    "    vs_inter_horizon_in[i,:] = zz.reshape(len(xx)*len(yy))[index]\n",
    "    OK = OrdinaryKriging(x_centroid_partition, y_centroid_partition, vs_horizon_fund[i], variogram_model=type_interp,nlags=3)\n",
    "    zz,ss = OK.execute('grid', xx, yy)\n",
    "    vs_inter_fund_horizon_in[i,:] = zz.reshape(len(xx)*len(yy))[index]\n",
    "OK = OrdinaryKriging(x_stations_in, y_stations_in, elevation_stations_in, variogram_model=type_interp,nlags=3)\n",
    "zz,ss = OK.execute('grid', xx, yy)\n",
    "ele_inter = zz.reshape(len(xx)*len(yy))[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对vs_inter进行平滑处理,用gaussain filter\n",
    "if flag_smooth == 1:\n",
    "    vs_inter_smooth = np.zeros((N,len(index)))\n",
    "    vs_inter_fund_smooth = np.zeros((N,len(index)))\n",
    "    for i in range(len(index)):\n",
    "        vs_inter_smooth[:,i] = gaussian_filter(vs_inter_horizon_in[:,i], sigma=2)\n",
    "        vs_inter_fund_smooth[:,i] = gaussian_filter(vs_inter_fund_horizon_in[:,i], sigma=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as npz\n",
    "z = np.arange(0,N*dz,dz)\n",
    "if flag_save == 1:\n",
    "    print('Save as npz')\n",
    "    filename = dir_inv + 'vs_inter.npz'\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    # save as npz\n",
    "    np.savez(filename,ele = ele_inter,vs_fund = vs_inter_fund_horizon_in,vs = vs_inter_horizon_in, x = x_inter_in,y = y_inter_in, depth = z,dz=dz,N=N,lon=lon_inter_in,lat=lat_inter_in)\n",
    "\n",
    "    if flag_smooth == 1:\n",
    "        filename = dir_inv + 'vs_inter_smooth.npz'\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        np.savez(filename,ele = ele_inter,vs_fund = vs_inter_fund_smooth,vs = vs_inter_smooth, x = x_inter_in,y = y_inter_in, depth = z,dz=dz,N=N,lon=lon_inter_in,lat=lat_inter_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
