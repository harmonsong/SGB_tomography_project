{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "#from ccfj import GetStationPairs\n",
    "from geopy.distance import great_circle\n",
    "#import shapefile\n",
    "import geopandas as gp\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.interpolate import griddata\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import plotlib\n",
    "from toollib_standard import mathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_project = 1 # 0--regular; 1--repartrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_project == 0:\n",
    "    file_project = 'a-project.yml'\n",
    "elif flag_project == 1:\n",
    "    file_project = 'a-project_repar.yml'\n",
    "elif flag_project == 2:\n",
    "    file_project = 'a-project_voro.yml'\n",
    "    \n",
    "with open(file_project, 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "\n",
    "#name_project = 'project/output_FJSJ_16-01/'               \n",
    "#name_project = 'project_repartrition/repartrition_01-03/'               \n",
    "#name_project = 'project_voronoi/voronoi_01-03/'         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_model_fund = dir_project + info_basic['rdir_model_fund']\n",
    "dir_model = dir_project + info_basic['rdir_model']\n",
    "dir_image = dir_project + info_basic['rdir_image']+'Vs_compare/'\n",
    "if not os.path.exists(dir_image):\n",
    "    os.makedirs(dir_image)\n",
    "dir_inv = dir_project + info_basic['rdir_inv_BGFS']\n",
    "dir_partition = dir_project + info_basic['rdir_partition']\n",
    "key_subworks = info_basic['key_subworks']\n",
    "M = len(key_subworks)\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = mathlib.GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist() \n",
    "lat_stations_all =  stainfo['latitude'].tolist() \n",
    "lon_stations_all =  stainfo['longitude'].tolist() \n",
    "elevation_stations_all = stainfo['elevation'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_partition = {}\n",
    "lat_stations_partition = {}\n",
    "lon_stations_partition = {}\n",
    "lat_centroid_partition = []\n",
    "lon_centroid_partition = []\n",
    "num_stations = []\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_partition[key] = stations_this\n",
    "    lat_stations_partition[key] = lat_stations_this.astype(float)\n",
    "    lon_stations_partition[key] = lon_stations_this.astype(float)\n",
    "    num_stations.append(len(stations_this))\n",
    "    lat_centroid_partition.append(np.mean(lat_stations_this.astype(float)))\n",
    "    lon_centroid_partition.append(np.mean(lon_stations_this.astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = np.load('clark_faults.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = np.column_stack((lon_stations_all, lat_stations_all))\n",
    "points = np.column_stack((lon_centroid_partition, lat_centroid_partition))\n",
    "hull = ConvexHull(points)\n",
    "polygon = Polygon(points[hull.vertices])\n",
    "index_sta = []\n",
    "lon_stations_in = []\n",
    "lat_stations_in = []\n",
    "lon_fault_in = []\n",
    "lat_fautl_in = []\n",
    "elevation_stations_in = []\n",
    "for i in range(len(lon_stations_all)):\n",
    "    if polygon.contains(Point(lon_stations_all[i], lat_stations_all[i])):\n",
    "        index_sta.append(i)\n",
    "        lon_stations_in.append(lon_stations_all[i])\n",
    "        lat_stations_in.append(lat_stations_all[i])\n",
    "        elevation_stations_in.append(elevation_stations_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affine_transform(original_points, target_points):\n",
    "    A_matrix = np.array([[original_points[0][0], original_points[0][1], 1, 0, 0, 0],\n",
    "                         [0, 0, 0, original_points[0][0], original_points[0][1], 1],\n",
    "                         [original_points[1][0], original_points[1][1], 1, 0, 0, 0],\n",
    "                         [0, 0, 0, original_points[1][0], original_points[1][1], 1],\n",
    "                         [original_points[2][0], original_points[2][1], 1, 0, 0, 0],\n",
    "                         [0, 0, 0, original_points[2][0], original_points[2][1], 1]])\n",
    "\n",
    "    A1_B1_C1 = np.array([target_points[0][0], target_points[0][1], target_points[1][0], target_points[1][1], target_points[2][0], target_points[2][1]])\n",
    "\n",
    "    coefficients = np.linalg.solve(A_matrix, A1_B1_C1)\n",
    "\n",
    "    affine_matrix = np.array([[coefficients[0], coefficients[1], coefficients[2]],\n",
    "                               [coefficients[3], coefficients[4], coefficients[5]],\n",
    "                               [0, 0, 1]])\n",
    "\n",
    "    return affine_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine transformation\n",
    "lon_stations_all_new = []\n",
    "lat_stations_all_new = []\n",
    "stalist_all_new = []\n",
    "elevation_stations_all_new = []\n",
    "for sta in stalist_all:\n",
    "    if int(sta[1:3]) <= 60:\n",
    "        lon_stations_all_new.append(lon_stations_all[stalist_all.index(sta)])\n",
    "        lat_stations_all_new.append(lat_stations_all[stalist_all.index(sta)])\n",
    "        stalist_all_new.append(sta)\n",
    "        elevation_stations_all_new.append(elevation_stations_all[stalist_all.index(sta)])\n",
    "refs = ['R0101','R6001','R6020']\n",
    "lon_refs = [lon_stations_all[stalist_all.index(ref)] for ref in refs]\n",
    "lat_refs = [lat_stations_all[stalist_all.index(ref)] for ref in refs]\n",
    "loc_refs = np.column_stack([lon_refs,lat_refs])\n",
    "loc_refs_new = np.array([[0,0],[600,0],[600,600]])\n",
    "\n",
    "affine_matrix = compute_affine_transform(loc_refs, loc_refs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all stations\n",
    "x_stations_all_new = []\n",
    "y_stations_all_new = []\n",
    "for i in range(len(lon_stations_all_new)):\n",
    "    loc_sta = np.array([lon_stations_all_new[i],lat_stations_all_new[i],1])\n",
    "    loc_sta_new = np.dot(affine_matrix,loc_sta)\n",
    "    x_stations_all_new.append(loc_sta_new[0])\n",
    "    y_stations_all_new.append(loc_sta_new[1])\n",
    "# In station\n",
    "x_stations_in = []\n",
    "y_stations_in = []\n",
    "for i in range(len(lon_stations_in)):\n",
    "    loc_sta = np.array([lon_stations_in[i],lat_stations_in[i],1])\n",
    "    loc_sta_new = np.dot(affine_matrix,loc_sta)\n",
    "    x_stations_in.append(loc_sta_new[0])\n",
    "    y_stations_in.append(loc_sta_new[1])\n",
    "# faults\n",
    "x_faults = {}\n",
    "y_faults = {}\n",
    "for i in range(len(faults)):\n",
    "    x_faults['clark'+str(i+1)] = []\n",
    "    y_faults['clark'+str(i+1)] = []\n",
    "    for j in range(len(faults['clark'+str(i+1)]['lon'])):\n",
    "        loc_fault = np.array([faults['clark'+str(i+1)]['lon'][j],faults['clark'+str(i+1)]['lat'][j],1])\n",
    "        loc_fault_new = np.dot(affine_matrix,loc_fault)\n",
    "        x_faults['clark'+str(i+1)].append(loc_fault_new[0])\n",
    "        y_faults['clark'+str(i+1)].append(loc_fault_new[1])\n",
    "# partitions\n",
    "x_centroid_partition = []\n",
    "y_centroid_partition = []\n",
    "for i in range(len(lon_centroid_partition)):\n",
    "    loc_centroid = np.array([lon_centroid_partition[i],lat_centroid_partition[i],1])\n",
    "    loc_centroid_new = np.dot(affine_matrix,loc_centroid)\n",
    "    x_centroid_partition.append(loc_centroid_new[0])\n",
    "    y_centroid_partition.append(loc_centroid_new[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_file = dir_inv + 'vs_inter.npz'\n",
    "inter = np.load(inter_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_fund = inter['vs_fund']\n",
    "vs = inter['vs']\n",
    "x_inter_in = inter['x']\n",
    "y_inter_in = inter['y']\n",
    "ele_inter_in = inter['ele']\n",
    "z = inter['depth']\n",
    "dz = inter['dz']\n",
    "N = inter['N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define traces\n",
    "traces = {}\n",
    "for i in range(4,16):\n",
    "    y = 30*(i-1)\n",
    "    traces[i] = np.zeros([2,2])\n",
    "    traces[i][0,0] = np.min(x_inter_in)\n",
    "    traces[i][0,1] = y\n",
    "    traces[i][1,0] = np.max(x_inter_in)\n",
    "    traces[i][1,1] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_points(tag_trace,x_inter_in,y_inter_in):\n",
    "    global traces\n",
    "    trace = traces[tag_trace]\n",
    "\n",
    "    x1 = trace[0,0]\n",
    "    y1 = trace[0,1]\n",
    "    x2 = trace[1,0]\n",
    "    y2 = trace[1,1]\n",
    "    A = (y2-y1)/(x2-x1)\n",
    "    B = -1\n",
    "    C = y2 - A*x2\n",
    "\n",
    "    flag_r = np.sqrt(( (y_inter_in[1]-y_inter_in[0])**2 + (x_inter_in[1]-x_inter_in[0])**2 ))/2\n",
    "    index = []\n",
    "    \n",
    "    for i in range(len(x_inter_in)):\n",
    "        x0 = x_inter_in[i]\n",
    "        y0 = y_inter_in[i]\n",
    "        d = abs(A*x0+B*y0+C)/math.sqrt(A**2+B**2)\n",
    "        if d <= flag_r:\n",
    "            index.append(i)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection_point(point1_line, point2_line, external_point):\n",
    "    # 计算直线的斜率\n",
    "    line_slope = (point2_line[1] - point1_line[1]) / (point2_line[0] - point1_line[0])\n",
    "\n",
    "    if line_slope == 0:\n",
    "        #print('slope == 0')\n",
    "        #print(external_point[1],point1_line[0])\n",
    "        return  point1_line[0],external_point[1]\n",
    "    # 计算直线的截距\n",
    "    line_intercept = point1_line[1] - line_slope * point1_line[0]\n",
    "\n",
    "    # 计算垂线的斜率\n",
    "    perpendicular_slope = -1 / line_slope\n",
    "\n",
    "    # 计算垂线的截距\n",
    "    perpendicular_intercept = external_point[1] - perpendicular_slope * external_point[0]\n",
    "\n",
    "    # 求解方程组以找到交点\n",
    "    intersection_x = (perpendicular_intercept - line_intercept) / (line_slope - perpendicular_slope)\n",
    "    intersection_y = line_slope * intersection_x + line_intercept\n",
    "\n",
    "    return intersection_x, intersection_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inter_verti = {}\n",
    "y_inter_verti = {}\n",
    "vs_inter_verti = {}\n",
    "vs_inter_fund_verti = {}   \n",
    "ele_verti = {}\n",
    "indexes = {}\n",
    "loc_starts = {}\n",
    "for tag in traces.keys():\n",
    "    print(tag)\n",
    "    #index = extract_points(tag,x_inter_in,y_inter_in)\n",
    "    yy = list(set(y_inter_in))\n",
    "    index_this = np.argmin(np.abs(yy-traces[tag][0][1]))\n",
    "    index = np.where(y_inter_in == yy[index_this])[0]\n",
    "    \n",
    "    indexes[tag] = index\n",
    "    x_inter_verti[tag] = np.array(x_inter_in)[index]\n",
    "    y_inter_verti[tag] = np.array(y_inter_in)[index]\n",
    "    \n",
    "    vs_inter_verti[tag] = []\n",
    "    vs_inter_fund_verti[tag] = []\n",
    "    ele_verti[tag] = []\n",
    "    loc_start = find_intersection_point(traces[tag][0], traces[tag][1], [x_inter_verti[tag][0], y_inter_verti[tag][0]])\n",
    "    loc_starts[tag] = loc_start\n",
    "    for i in range(N):\n",
    "        vs_inter_verti[tag].append(vs[i,index])\n",
    "        vs_inter_fund_verti[tag].append(vs_fund[i,index])\n",
    "    ele_verti[tag].append(np.array(ele_inter_in)[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断两条线段是否相交\n",
    "def cross_product(p1, p2, p3):\n",
    "    \"\"\"计算叉积\"\"\"\n",
    "    return (p2[0] - p1[0]) * (p3[1] - p1[1]) - (p2[1] - p1[1]) * (p3[0] - p1[0])\n",
    "\n",
    "def on_segment(p1, p2, p3):\n",
    "    \"\"\"检查p2是否在以p1和p3为端点的线段上\"\"\"\n",
    "    return min(p1[0], p3[0]) <= p2[0] <= max(p1[0], p3[0]) and min(p1[1], p3[1]) <= p2[1] <= max(p1[1], p3[1])\n",
    "\n",
    "def segments_intersect(p1, q1, p2, q2):\n",
    "    \"\"\"检查由(p1, q1)和(p2, q2)形成的两条线段是否相交\"\"\"\n",
    "    # 检查线段的方向\n",
    "    o1 = cross_product(p1, q1, p2)\n",
    "    o2 = cross_product(p1, q1, q2)\n",
    "    o3 = cross_product(p2, q2, p1)\n",
    "    o4 = cross_product(p2, q2, q1)\n",
    "\n",
    "    # 如果两条线段的方向相异，则它们相交\n",
    "    if o1 * o2 < 0 and o3 * o4 < 0:\n",
    "        return True\n",
    "    # 特殊情况处理\n",
    "    if o1 == 0 and on_segment(p1, p2, q1):\n",
    "        return True\n",
    "    if o2 == 0 and on_segment(p1, q2, q1):\n",
    "        return True\n",
    "    if o3 == 0 and on_segment(p2, p1, q2):\n",
    "        return True\n",
    "    if o4 == 0 and on_segment(p2, q1, q2):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def plot_segments(p1, q1, p2, q2):\n",
    "    plt.plot([p1[0], q1[0]], [p1[1], q1[1]], color='blue', label='Segment 1')\n",
    "    plt.plot([p2[0], q2[0]], [p2[1], q2[1]], color='red', label='Segment 2')\n",
    "\n",
    "    intersection = segments_intersect(p1, q1, p2, q2)\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Segments Intersection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fault_inter(tag_trace):\n",
    "    global traces\n",
    "    global faults\n",
    "    global x_faults\n",
    "    global y_faults\n",
    "    global x_inter_verti\n",
    "    global y_inter_verti\n",
    "\n",
    "    x_fault_inter = []\n",
    "    y_fault_inter = []\n",
    "\n",
    "    for key in faults.keys():\n",
    "        x_fault = x_faults[key]\n",
    "        y_fault = y_faults[key]\n",
    "        index_fault_start = np.argmin(y_fault)\n",
    "        index_fault_end = np.argmax(y_fault)\n",
    "        p1 = [x_fault[index_fault_start], y_fault[index_fault_start]]\n",
    "        q1 = [x_fault[index_fault_end], y_fault[index_fault_end]]\n",
    "        index_trace_start = np.argmin(x_inter_verti[tag_trace])\n",
    "        index_trace_end = np.argmax(x_inter_verti[tag_trace])\n",
    "        p2 = [x_inter_verti[tag_trace][index_trace_start], y_inter_verti[tag_trace][index_trace_start]]\n",
    "        q2 = [x_inter_verti[tag_trace][index_trace_end], y_inter_verti[tag_trace][index_trace_end]]\n",
    "        \n",
    "        if segments_intersect(p1, q1, p2, q2):\n",
    "            # find the nearest fault point\n",
    "            x_trace = x_inter_verti[tag_trace]\n",
    "            y_trace = y_inter_verti[tag_trace]\n",
    "\n",
    "            min_distance = float('inf')\n",
    "\n",
    "            for lon1, lat1 in zip(x_fault, y_fault):\n",
    "                for lon2, lat2 in zip(x_trace, y_trace):\n",
    "                    distance = math.sqrt((lon1 - lon2)**2 + (lat1 - lat2)**2)\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        x_fault_near = lon1\n",
    "                        y_fault_near = lat1\n",
    "                        x_trace_near = lon2\n",
    "                        y_trace_near = lat2\n",
    "                    \n",
    "            x_fault_inter.append( x_trace_near )\n",
    "            y_fault_inter.append( y_trace_near )\n",
    "        \n",
    "        \"\"\"\n",
    "        intersection = find_lines_inter(p1, q1, p2, q2)\n",
    "        if intersection is not None:\n",
    "            lon_fault_inter.append(intersection[0])\n",
    "            lat_fault_inter.append(intersection[1])\n",
    "        \"\"\"\n",
    "    return x_fault_inter, y_fault_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_points(num_points):\n",
    "    # 生成随机散点\n",
    "    points = np.random.rand(num_points, 2) * 10  # 在[0, 10]范围内生成均匀分布的随机点\n",
    "    return points\n",
    "\n",
    "def plot_points(ax, points, color='blue'):\n",
    "    # 绘制散点\n",
    "    ax.scatter(points[:, 0], points[:, 1], color=color)\n",
    "\n",
    "def minimum_bounding_rectangle(points):\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    hull_points = points[hull.vertices]\n",
    "    \n",
    "    # 对凸包进行拟合\n",
    "    rect = Polygon(hull_points).minimum_rotated_rectangle\n",
    "    coords = np.array(rect.exterior.coords.xy)\n",
    "    \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_min = 0.45\n",
    "v_max = 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15),dpi=100)\n",
    "ax = fig.add_subplot(1,1,1,projection = '3d')\n",
    "\n",
    "for j in range(len(faults)):\n",
    "    ax.plot3D(x_faults['clark'+str(j+1)], y_faults['clark'+str(j+1)], np.zeros_like(x_faults['clark'+str(j+1)]), 'k--',zorder=20)\n",
    "#ax.view_init(elev=30, azim=-140)\n",
    "y_sticks = []\n",
    "for i in list(traces.keys())[::]:\n",
    "    y_sticks.append(y_inter_verti[i][0])\n",
    "for tag in list(traces.keys())[::]:\n",
    "    vs_this = vs_inter_verti[tag]\n",
    "    x = x_inter_verti[tag]\n",
    "    y = y_inter_verti[tag]\n",
    "    z0 = z*1e-3\n",
    "    X,Z = np.meshgrid(x,z0)\n",
    "    norm = Normalize(vmin=v_min, vmax=v_max)\n",
    "    im = ax.plot_surface(x,y,z,rstride=1, cstride=1, facecolors=plt.cm.gist_rainbow(norm(vs_this)), alpha = 0.7,shade=False,zorder=1)\n",
    "\n",
    "ax.tick_params(axis='z', which='major', pad=-10)  # 调整刻度文本与轴的距离\n",
    "#plt.subplots_adjust(left=0.3, right=0.9, top=0.9, bottom=0.1)\n",
    "ax.view_init(elev=45, azim=-150)\n",
    "\n",
    "ax.set_box_aspect([6,10,2])#\n",
    "#ax.axis('off')\n",
    "ax.set_xlabel('X /m')\n",
    "ax.set_ylabel('Y /m')\n",
    "ax.set_zlabel(\"Z /km\", rotation=90,labelpad=-6)\n",
    "#ax.zaxis.labelpad=-5\n",
    "#ax.set_xticks(set(x))\n",
    "ax.set_xlim([100,550])\n",
    "ax.set_yticks(y_sticks)\n",
    "ax.set_ylim([min(y_sticks), max(y_sticks)])\n",
    "ax.set_zticks([0,0.1,0.2])\n",
    "ax.set_zlim(ax.get_zlim()[::-1])\n",
    "plt.tick_params(pad=0.3)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('test.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
