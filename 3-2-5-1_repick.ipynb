{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import geopandas as gp\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../tools_F-J/')\n",
    "from toollib_standard import maplib\n",
    "from toollib_standard import mathlib\n",
    "from toollib_standard import filelib\n",
    "from toollib_standard import plotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a-project_repar.yml', 'r', encoding='utf-8') as f:\n",
    "    proj = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "name_project = proj['name']\n",
    "#name_project = 'project_repartrition/output_repar_03-01/'               # Harmon server\n",
    "name_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_config.yml', 'r', encoding='utf-8') as f:\n",
    "    dir_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "dir_project_workspace = dir_config['dir_project_workspace']\n",
    "dir_CC_workspace = dir_config['dir_CC_workspace']\n",
    "print('dir_CC_workspace: ', dir_CC_workspace)\n",
    "print('dir_project_workspace: ', dir_project_workspace)\n",
    "dir_project = os.path.join(dir_project_workspace, name_project)\n",
    "print('dir_project: ', dir_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir_project_probes = info_basic['rdir_project_probes']\n",
    "rdir_project_targets = info_basic['rdir_project_targets']\n",
    "dir_project_probes = os.path.join(dir_project_workspace, rdir_project_probes)\n",
    "dir_project_targets = os.path.join(dir_project_workspace, rdir_project_targets)\n",
    "print('dir_project_probes: ', dir_project_probes)\n",
    "print('dir_project_targets: ', dir_project_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dir_project_probes+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic_probes = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project_probes+'Basic_info.npy'\n",
    "info_basic_bi_probes = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary\n",
    "\n",
    "filename = dir_project_targets+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic_targets = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "\n",
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = info_basic_probes['key_subworks']\n",
    "targets = info_basic_targets['key_subworks']\n",
    "key_subworks = info_basic['key_subworks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalistname_all = info_basic_probes['stalistname_all']\n",
    "stainfo = pd.read_excel(stalistname_all)\n",
    "nsta_all = len(stainfo.iloc[:,0])\n",
    "StationPairs_all = mathlib.GetStationPairs(nsta_all)\n",
    "nPairs_all = int(len(StationPairs_all)/2)\n",
    "stalist_all = stainfo['Station'].tolist()\n",
    "lat_stations_all = stainfo['latitude'].tolist() \n",
    "lon_stations_all = stainfo['longitude'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_partition = dir_project + info_basic['rdir_partition']\n",
    "dir_partition_probes = dir_project_probes + info_basic_probes['rdir_partition']\n",
    "dir_partition_targets = dir_project_targets + info_basic_targets['rdir_partition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_probes = {}\n",
    "lat_probes = {}\n",
    "lon_probes = {}\n",
    "for probe in probes:\n",
    "    filepath = dir_partition_probes + str(probe) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_probes[probe] = stations_this\n",
    "    lat_probes[probe] = lat_stations_this.astype(float)\n",
    "    lon_probes[probe] = lon_stations_this.astype(float)\n",
    "\n",
    "stations_targets = {}\n",
    "lat_targets = {}\n",
    "lon_targets = {}\n",
    "for target in targets:\n",
    "    filepath = dir_partition_targets + str(target) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations_targets[target] = stations_this\n",
    "    lat_targets[target] = lat_stations_this.astype(float)\n",
    "    lon_targets[target] = lon_stations_this.astype(float)\n",
    "\n",
    "stations = {}\n",
    "lat = {}\n",
    "lon = {}\n",
    "for key in key_subworks:\n",
    "    filepath = dir_partition + str(key) + '.txt'\n",
    "    stations_this, lat_stations_this, lon_stations_this = np.loadtxt(filepath, dtype='str' , unpack=True)\n",
    "    stations[key] = stations_this\n",
    "    lat[key] = lat_stations_this.astype(float)\n",
    "    lon[key] = lon_stations_this.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = np.load('clark_faults.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_image = dir_project+info_basic['rdir_image']+'repick/'\n",
    "if not os.path.exists(dir_image):\n",
    "    os.makedirs(dir_image)\n",
    "dir_disp_probes  = dir_project_probes + info_basic_probes['rdir_autopick'] \n",
    "print('dir_image: ', dir_image)\n",
    "print('dir_disp: ', dir_disp_probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accept_all = info_basic_bi['probe_accept']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probes(target):\n",
    "    global dir_project\n",
    "    global dir_project_probes\n",
    "    global dir_project_targets\n",
    "    global probes\n",
    "    global stations_targets\n",
    "\n",
    "    stas = stations_targets[target]\n",
    "    # 生成sta的包络\n",
    "    #hull_sta = ConvexHull(stainfo_targets[key_subwork][['Longitude','Latitude']])\n",
    "\n",
    "    stas2key = [sta[1:3]+'-'+sta[3:] for sta in stas]\n",
    "    probes_this = []\n",
    "    for sta in stas2key:\n",
    "        if sta in probes:\n",
    "            probes_this.append(sta)\n",
    "    return probes_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(lon_this,lat_this,lon_stations_all,lat_stations_all):\n",
    "    lon_stations_all = np.array(lon_stations_all)\n",
    "    lat_stations_all = np.array(lat_stations_all)\n",
    "    lon_stations_all = lon_stations_all.reshape(-1,1)\n",
    "    lat_stations_all = lat_stations_all.reshape(-1,1)\n",
    "    dist = np.sqrt((lon_stations_all-lon_this)**2+(lat_stations_all-lat_this)**2)\n",
    "    min_dist = np.min(dist)\n",
    "    index = np.where(dist==min_dist)[0][0]\n",
    "    #print(lon_stations_all)\n",
    "    #print(index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lat_lon(new_subwork):\n",
    "    global stations_accept_all_new\n",
    "    global lon_centroid_all_new\n",
    "    global lat_centroid_all_new\n",
    "    global stations_accept_all\n",
    "    global lon_centroid_all\n",
    "    global lat_centroid_all\n",
    "\n",
    "    target = new_subwork.split('--')[1]\n",
    "    stations_accept_all_new[new_subwork] = set(stations_accept_all[target])\n",
    "    lon_centroid_all_new[new_subwork] = lon_centroid_all[target]\n",
    "    lat_centroid_all_new[new_subwork] = lat_centroid_all[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_filter(target):\n",
    "    global flag_err\n",
    "    global flag_plot\n",
    "    global dir_image\n",
    "    global flag_pass\n",
    "    global ds_probe_all\n",
    "    global targets\n",
    "    global probes\n",
    "    global lon_stations_all\n",
    "    global lat_stations_all\n",
    "    global lon_sta\n",
    "    global stations_accept_all\n",
    "    global lon_centroid_all \n",
    "    global lat_centroid_all \n",
    "    global probe_accept_all\n",
    "    global probe_ref_all\n",
    "    global f_range\n",
    "    # extract all disp\n",
    "    probes_this = extract_probes(target)\n",
    "    stations = set()\n",
    "    lon_centroid_probe = []\n",
    "    lat_centroid_probe = []\n",
    "    for probe in probes_this:\n",
    "        stations_this = stations_probes[probe]\n",
    "        stations = stations.union(set(stations_this))\n",
    "        lon_centroid_probe.append(np.mean( lon_probes[probe]))\n",
    "        lat_centroid_probe.append(np.mean( lat_probes[probe]))\n",
    "    stations = list(stations)\n",
    "    lon_stations_this = []\n",
    "    lat_stations_this = []\n",
    "    for sta in stations:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_this.append(lon_stations_all[index])\n",
    "        lat_stations_this.append(lat_stations_all[index])\n",
    "\n",
    "    # 读取全部autopick dispersion curves\n",
    "    disp_all = {}\n",
    "    for probe in probes_this:\n",
    "        file_ds = dir_project_probes  + info_basic_probes['rdir_autopick'] + 'autopick_'+probe+'.txt'\n",
    "        disp = np.loadtxt(file_ds,delimiter=',')\n",
    "        index = np.where((disp[:,0]>=f_range[0]) & (disp[:,0]<=f_range[1]))[0]\n",
    "        disp_all[probe] = disp[index,:]\n",
    "\n",
    "    # find central probe and cluster based on it\n",
    "    lon_centroid_this = np.mean(lon_stations_this)\n",
    "    lat_centroid_this = np.mean(lat_stations_this)\n",
    "    index = find_nearest(lon_centroid_this,lat_centroid_this,lon_centroid_probe,lat_centroid_probe)\n",
    "    probe_ref = probes_this[index]\n",
    "    \n",
    "    # original partition\n",
    "    sta_original = set()\n",
    "    for probe in probes_this:\n",
    "        sta_original = sta_original.union(stations_probes[probe])\n",
    "    sta_original = list(sta_original)\n",
    "    lon_stations_original = []\n",
    "    lat_stations_original = []\n",
    "    for sta in sta_original:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_original.append(lon_stations_all[index])\n",
    "        lat_stations_original.append(lat_stations_all[index])\n",
    "    \n",
    "    # calculate relative error between all probes and reference probe\n",
    "    err = []\n",
    "    f_sample = np.linspace(f_range[0],f_range[1],200,endpoint=False)\n",
    "    f_pick_ref = np.loadtxt(dir_disp_probes+'autopick_'+probe_ref+'.txt',delimiter=',')[:,0]\n",
    "    c_pick_ref0 = np.loadtxt(dir_disp_probes+'autopick_'+probe_ref+'.txt',delimiter=',')[:,1]\n",
    "    f = interpolate.interp1d(f_pick_ref,c_pick_ref0,kind='cubic',fill_value='extrapolate')\n",
    "    c_pick_ref = f(f_sample)\n",
    "\n",
    "    c_pick = {}\n",
    "    for probe in probes_this:\n",
    "        c_pick[probe] = []\n",
    "        file_ds = dir_project_probes  + info_basic_probes['rdir_autopick'] + 'autopick_'+probe+'.txt'\n",
    "        disp = np.loadtxt(file_ds,delimiter=',')\n",
    "        index = np.where((disp[:,0]>=f_range[0]) & (disp[:,0]<=f_range[1]))[0]\n",
    "        disp = disp[index,:]\n",
    "        f_this = disp[:,0]\n",
    "        c_this = disp[:,1]\n",
    "        # 生成插值函数,可以外推\n",
    "        #f = interpolate.interp1d(f_this,c_this,kind='cubic')\n",
    "        f = interpolate.interp1d(f_this,c_this,kind='cubic',fill_value='extrapolate')\n",
    "        c_pick[probe] = f(f_sample)\n",
    "\n",
    "        max_disp = np.sum( np.maximum(np.array(c_pick_ref),np.array(c_pick[probe])) )\n",
    "        abs_err = np.sum( np.abs(np.abs(np.array(c_pick_ref))-np.abs(np.array(c_pick[probe]))) )\n",
    "        err.append(abs_err/max_disp) \n",
    "        \n",
    "    # pass or not\n",
    "    index_pass = np.array(err) < flag_err\n",
    "    probe_pass = np.array(probes_this)[index_pass]\n",
    "    lon_centroid_pass = np.array(lon_centroid_probe)[index_pass]\n",
    "    lat_centroid_pass = np.array(lat_centroid_probe)[index_pass]\n",
    "    err_pass = np.array(err)[index_pass]\n",
    "\n",
    "    # new partition\n",
    "    sta_pass = set()\n",
    "    lon_stations_pass = []\n",
    "    lat_stations_pass = []\n",
    "    for probe in probe_pass:\n",
    "        sta_pass = sta_pass.union(stations_probes[probe])\n",
    "    sta_pass = list(sta_pass)\n",
    "    for sta in sta_pass:\n",
    "        index = stalist_all.index(sta)\n",
    "        lon_stations_pass.append(lon_stations_all[index])\n",
    "        lat_stations_pass.append(lat_stations_all[index])\n",
    "    stations_accept_all[target] = sta_pass\n",
    "    lon_centroid_all[target] = np.mean(lon_stations_pass)\n",
    "    lat_centroid_all[target] = np.mean(lat_stations_pass)\n",
    "    if flag_plot:\n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "    \n",
    "        ax = fig.add_subplot(231)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_probe,lat_centroid_probe,c=err,s=20,cmap='jet')\n",
    "        im.set_clim(vmin=0,vmax=0.1)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target + ' error between f='+str(f_range[0])+' and f='+str(f_range[1]))\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax = fig.add_subplot(232)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        im = ax.scatter(lon_centroid_pass,lat_centroid_pass,c=err_pass,s=20,cmap='jet')\n",
    "        im.set_clim(vmin=0,vmax=0.1)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target + ' selected probes')\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "\n",
    "        ax = fig.add_subplot(233)\n",
    "        ax.scatter(lon_stations_all,lat_stations_all,c='k',s=1)\n",
    "        for i in range(len(faults)):\n",
    "            ax.plot(faults['clark'+str(i+1)]['lon'], faults['clark'+str(i+1)]['lat'], 'k--',lw=1)\n",
    "        ax.scatter(lon_stations_original,lat_stations_original,c = 'g',s=20,label = 'original partitions')\n",
    "        # 画空心圆\n",
    "        ax.scatter(lon_stations_pass,lat_stations_pass,c = None,s=30,marker='o',edgecolors='r',label = 'new partitions')\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(target+' partition comparison')\n",
    "        ax.legend()\n",
    "\n",
    "        ax = fig.add_subplot(223)\n",
    "        for probe in probes_this:\n",
    "            if probes_this.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.',label='all probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.')\n",
    "        ax.plot(f_sample,c_pick_ref,'r',lw=2,label='central probe')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.set_title(target+\" all probes' dispersion curves\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim(f_range[0],f_range[1])\n",
    "        ax.set_ylim(0.2,2)\n",
    "\n",
    "        ax = fig.add_subplot(224)\n",
    "        for probe in probes_this:\n",
    "            if probes_this.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.',label='all probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='k',s=1,marker='.')\n",
    "        probe_pass = list(probe_pass)\n",
    "        for probe in probe_pass:\n",
    "            if probe_pass.index(probe) == 0:\n",
    "                ax.scatter(f_sample,c_pick[probe],c='g',s=2,marker='.',label='picked probes')\n",
    "            ax.scatter(f_sample,c_pick[probe],c='g',s=2,marker='.')\n",
    "        ax.plot(f_sample,c_pick_ref,'r',lw=2,label='central probe')\n",
    "        ax.set_xlabel('frequency (Hz)')\n",
    "        ax.set_ylabel('phase velocity (km/s)')\n",
    "        ax.set_title(target+\" dispersion curves accepted\")\n",
    "        ax.set_xlim(f_range[0],f_range[1])\n",
    "        ax.set_ylim(0.2,2)\n",
    "        ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_image+'K-S_filter_'+target+'.png',dpi=100)\n",
    "        #plt.close()\n",
    "    \n",
    "    print('cluster_filter: ', target + ' finished, '+str(targets.index(target)+1)+'/'+str(len(targets)))\n",
    "    probe_accept_all[target] = probe_pass\n",
    "    probe_ref_all[target] = probe_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_probe_all = {}\n",
    "for probe in info_basic_probes['key_subworks']:\n",
    "    file_ds = dir_project_probes  + info_basic_probes['rdir_ds'] + 'ds_'+probe+'.h5'\n",
    "    ds = h5py.File(file_ds,'r')\n",
    "    ds_probe_all[probe] = ds['ds_remove'][0]\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_repick =[590]\n",
    "filename = dir_project+'Basic_info.yml'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    info_basic = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "key_subworks = []\n",
    "for num in nums_repick:\n",
    "    tag = str(num)\n",
    "    for key_subwork in info_basic['key_subworks']:\n",
    "        if tag == key_subwork.split('--')[0]:\n",
    "            key_subworks.append(key_subwork)\n",
    "key_subworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_err = 0.02\n",
    "flag_plot = 1\n",
    "f_range = [10,14]\n",
    "#f_range = [14,18]\n",
    "flag_plot = 1\n",
    "nThreads = 40\n",
    "probe_accept_all = {}\n",
    "stations_accept_all = {}\n",
    "probe_ref_all = {}\n",
    "lon_centroid_all = {}\n",
    "lat_centroid_all = {}\n",
    "for key_subwork in key_subworks:\n",
    "    cluster_filter(key_subwork.split('--')[1])\n",
    "\n",
    "if 'key_subworks_repick' in info_basic.keys():\n",
    "    key_subworks_repick = set(info_basic['key_subworks_repick'])\n",
    "    key_subworks_repick = key_subworks_repick.union(set(key_subworks))\n",
    "    key_subworks_repick = list(key_subworks_repick)\n",
    "else:\n",
    "    key_subworks_repick = key_subworks\n",
    "info_basic['key_subworks_repick'] = key_subworks_repick\n",
    "\n",
    "stations_accept_all_new = {}\n",
    "lon_centroid_all_new = {} \n",
    "lat_centroid_all_new = {}\n",
    "for key_subwork in key_subworks:\n",
    "    check_lat_lon(key_subwork)\n",
    "\n",
    "filename_bi = dir_project+'Basic_info.npy'\n",
    "info_basic_bi = np.load(filename_bi, allow_pickle='TRUE').item()      # setting dictionary\n",
    "\n",
    "probe_accept_origin = info_basic_bi['probe_accept']\n",
    "key_probe = info_basic_bi['probe']\n",
    "for key_subwork in key_subworks:\n",
    "    target = key_subwork.split('--')[1]\n",
    "    probe_accept_origin[target] = probe_accept_all[target]\n",
    "    probes_this = extract_probes(target)\n",
    "    key_probe[key_subwork] = probes_this\n",
    "info_basic_bi['probe_accept'] = probe_accept_origin\n",
    "info_basic_bi['probe'] = key_probe\n",
    "\n",
    "for key in key_subworks:\n",
    "    Station = list(stations_accept_all_new[key_subwork])\n",
    "    lat = [lat_stations_all[stalist_all.index(station)] for station in stations_accept_all_new[key_subwork]]\n",
    "    lon =  [lon_stations_all[stalist_all.index(station)] for station in stations_accept_all_new[key_subwork]]\n",
    "    filename = dir_partition + key_subwork + '.txt'\n",
    "    np.savetxt(filename, np.array([Station, lat, lon]).T, fmt='%s', delimiter=' ', header='Station lat lon')\n",
    "\n",
    "with open(dir_project+'Basic_info.yml', 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(data=info_basic, stream=f, allow_unicode=True)\n",
    "np.save(dir_project + 'Basic_info.npy', info_basic_bi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccfj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
